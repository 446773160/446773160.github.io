<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>机器学习 | 山不让尘，川不辞盈</title><meta name="author" content="山不让尘，川不辞盈"><meta name="copyright" content="山不让尘，川不辞盈"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习算法">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="山不让尘，川不辞盈">
<meta property="og:description" content="机器学习算法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg">
<meta property="article:published_time" content="2022-10-21T07:00:00.000Z">
<meta property="article:modified_time" content="2022-12-07T08:19:18.204Z">
<meta property="article:author" content="山不让尘，川不辞盈">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 山不让尘，川不辞盈","link":"链接: ","source":"来源: 山不让尘，川不辞盈","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-07 16:19:18'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="山不让尘，川不辞盈" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F14071825039%2F641&amp;refer=http%3A%2F%2Finews.gtimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668341762&amp;t=8f1764bb17a8a475a7658fd817bf1965" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">山不让尘，川不辞盈</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-21T07:00:00.000Z" title="发表于 2022-10-21 15:00:00">2022-10-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-07T08:19:18.204Z" title="更新于 2022-12-07 16:19:18">2022-12-07</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">21.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>74分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><p><strong>本文介绍机器学习算法，全文采用python代码去写且使用numpy,pandas以及matplotlib。机器学习算法采用sklearn框架去写，本文也会介绍机器学习框架的算法。如果基础不好的同学请移步去学习python基础。</strong></p>
<h1 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h1><p><strong>维基百科：</strong></p>
<p>机器学习是近20多年兴起的一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与统计推断学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。</p>
<p><strong>网络教学信息</strong></p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">斯坦福机器学习</span><br><span class="line">http://v.163.com/special/opencourse/machinelearning.html</span><br><span class="line">CMU 机器学习课程</span><br><span class="line">http://www.cs.cmu.edu/~epxing/Class/10715/  </span><br><span class="line">http://www.cs.cmu.edu/~epxing/Class/10708/  视频</span><br><span class="line">http://www.cs.cmu.edu/~epxing/Class/10701</span><br><span class="line">https://sites.google.com/site/10601a14spring/syllabus </span><br><span class="line">http://wenku.baidu.com/course/view/49e8b8f67c1cfad6195fa705</span><br></pre></td></tr></table></figure>

<p><strong>相关学术文章下载资源</strong></p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html</span><br><span class="line">CV:http://www.cvpapers.com/index.html; </span><br><span class="line"><span class="attribute">NIPS</span><span class="punctuation">: </span>http://books.nips.cc/; </span><br><span class="line">JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  </span><br></pre></td></tr></table></figure>

<p><strong>机器学习的定义</strong></p>
<p>机器学习有下面几种定义：<br>“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。<br>“机器学习是对能通过经验自动改进的计算机算法的研究”。<br>“机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。”<br>英文定义：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p><strong>机器学习的应用</strong></p>
<ul>
<li>数据挖掘</li>
<li>计算机视觉</li>
<li>自然语言处理</li>
<li>生物特征识别</li>
<li>搜索引擎</li>
<li>医学诊断</li>
<li>检测信用卡欺诈</li>
<li>证券市场分析</li>
<li>DNA序列测序</li>
<li>语音和手写识别</li>
<li>战略游戏</li>
<li>机器人</li>
</ul>
<p><strong>机器学习新的方向</strong></p>
<ul>
<li>集成学习</li>
<li>可扩展机器学习（对大数据集、高维数据的学习等）</li>
<li>强化学习</li>
<li>迁移学习</li>
<li>概率网络</li>
<li>深度学习</li>
</ul>
<p><strong>机器学习和数据挖掘的关系</strong></p>
<ul>
<li>机器学习是数据挖掘的重要工具。</li>
<li>数据挖掘不仅仅要研究、拓展、应用一些机器学习方法，还要通过许多非机器学习技术解决数据仓储、大规模数据、数据噪音等等更为实际的问题。</li>
<li>机器学习的涉及面更宽，常用在数据挖掘上的方法通常只是“从数据学习”，然则机器学习不仅仅可以用在数据挖掘上，一些机器学习的子领域甚至与数据挖掘关系不大，例如增强学习与自动控制等等。</li>
<li>数据挖掘试图从海量数据中找出有用的知识。</li>
<li>大体上看，数据挖掘可以视为机器学习和数据库的交叉，它主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。</li>
</ul>
<p><strong>机器学习相关学术期刊和会议</strong></p>
<p><strong>机器学习</strong><br>    学术会议：NIPS、ICML、ECML和COLT，<br>    学术期刊：《Machine Learning》和《Journal of Machine Learning Research》<br><strong>数据挖掘</strong><br>    学术会议：SIGKDD、ICDM、SDM、PKDD和PAKDD<br>    学术期刊：《Data Mining and Knowledge Discovery》和《IEEE Transactions on Knowledge and Data Engineering》<br><strong>人工智能</strong><br>    学术会议：IJCAI和AAAI、<br><strong>数据库</strong><br>    学术会议：SIGMOD、VLDB、ICDE，<br><strong>其它一些顶级期刊如</strong><br>    《Artificial Intelligence》、<br>    《Journal of Artificial Intelligence Research》、<br>    《IEEE Transactions on Pattern Analysis and Machine Intelligence》、<br>    《Neural Computation》等也经常发表机器学习和数据挖掘方面的论文</p>
<h1 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h1><p>统计学习或机器学习一般包括<strong>监督学习</strong>、<strong>无监督学习</strong>、<strong>强化学习</strong>。有时还包括<strong>半监督学习</strong>、<strong>主动学习</strong>。</p>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>  <strong>监督学习（supervised learning）</strong> 是指从<strong>标注数据</strong>中学习预测模型的机器学习问题。<strong>标注数据</strong>表示输入输出的对应关系，<strong>预测模型</strong>对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的映射的统计规律。</p>
<p>（1）<strong>输入空间、输出空间和特征空间：</strong><br>          输入空间：输入所有可能取值的集合<br>          输出空间：输出所有可能取值的集合<br>          特征空间：所有特征向量存在的空间<br>  <strong>注1：</strong>输入与输出空间可以是有限元素的集合，也可以是整个欧式空间；输入空间与输出空间可以是同一个空间，也可以是不同的空间；通常输出空间远远小于输入空间。<br>  <strong>注2：</strong>特征空间的每一维对应一个特征。有时假设输入空间与特征空间为相同的空间，对它们不予区分；有时假设输入空间与特征空间为不同的空间，将实例从输入空间映射到特征空间。<br>（2）<strong>联合概率分布</strong><br>  监督学习假设输入与输出的随机变量X 和Y 遵循联合概率分布P ( X , Y ) 。P ( X , Y ) 表示分布函数，或分布密度函数。<br>  统计学习假设数据存在一定的统计规律，训练数据与测试数据被看作是依联合概率分布P ( X , Y ) 独立同分布产生的。<br>（3）<strong>假设空间</strong><br>  模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。假设空间也即意味着监督学习所要学习的范围。<br>（4）<strong>问题的形式化</strong><br>  监督学习分为学习和预测两个过程，由学习系统与预测系统共同完成。<br><img src="https://img-blog.csdnimg.cn/c8c90cfa48474a92b6c61d157376e940.png#pic_center" alt="img"></p>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p> 无监督学习（unsupervised learning） 是指从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据，预测模型表示数据的类别、转换或概率。无监督学习的本质是学习数据中的统计规律或潜在结构。<br>  无监督学习可用于对已有数据的分析，也可用于对未来数据的预测。它和监督学习有类似的流<br><img src="https://img-blog.csdnimg.cn/d4ffd967656844ad93ba49b905ebe50d.png#pic_center" alt="img"></p>
<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>​        <strong>强化学习（reinforcement learning）</strong> 是指<strong>智能系统与环境的连续互动</strong>中学习最优行为策略的机器学习问题。强化学习的本质是学习最优的序贯决策。<br>  强化学习过程中，<code>智能系统不断地试错，以达到学习最优策略的目的</code>。智能系统与环境的互动如图所示</p>
<p><img src="https://img-blog.csdnimg.cn/2bed1450aac74b85854540d424e3d58b.png#pic_center" alt="img"></p>
<h2 id="半监督学习与主动学习"><a href="#半监督学习与主动学习" class="headerlink" title="半监督学习与主动学习"></a>半监督学习与主动学习</h2><p>​        <strong>半监督学习（semi-supervised learning）</strong> 是指利用标注数据和未标注数据学习预测模型的机器学习问题。半监督学习旨在利用未标注数据中的信息，辅助标注数据进行监督学习，以较低的成本达到较好的学习效果。<br>  <strong>主动学习（active learning）</strong> 是指机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。主动学习旨在找出对学习最有帮助的实例让教师标注，以较小的标注代价，达到较好的学习效果。</p>
<h1 id="机器学习方法三要素"><a href="#机器学习方法三要素" class="headerlink" title="机器学习方法三要素"></a>机器学习方法三要素</h1><p>机器学习方法由<strong>模型</strong>、<strong>策略</strong>和<strong>算法</strong>三要素构成</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>​        机器学习首要考虑的问题是学习什么样的模型。在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。</p>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>​        有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。<br>（1）损失函数和风险函数<br>  损失函数：度量模型一次预测的好坏<br>  风险函数：度量平均意义下模型预测的好坏<br>由于模型的输入、输出( X , Y ) 是随机变量，遵循联合分布P ( X , Y ) ，所以损失函数的期望是：</p>
<p><img src="https://raw.githubusercontent.com/446773160/Picbed/main/blog_images20221021181635.png" alt="img"></p>
<p>​        这是理论模型f ( X )关于联合分布P ( X , Y ) 的平均意义下的损失，称为风险函数或期望损失。<br>实际上，联合分布P ( X , Y )是未知的，因为Rexp(f)是不能直接计算的，但根据大数定律，当样本容量N趋于无穷时，经验风险Remp(f)趋于期望风险Rexp(f)<strong>。</strong> 因为很自然的一个想法，即用经验风险估计期望风险。模型f ( X ) 关于训练数据集的平均损失即为经验风险或经验损失，记作Remp</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021182719.png?raw=true" alt="blog_images20221021182719.png"></p>
<p>但由于现实中训练样本数目有限，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。</p>
<p>（2）经验风险最小化与结构风险最小化<br>  为了求解到最优的模型，在监督学习中经常采用经验风险最小化和结构风险最小化这两个基本策略来选择模型。<br>  经验风险最小化（empirical risk minimization,ERM） 的策略认为，经验风险最小的模型即最优的模型，上述问题可转化为求解如下的最优化问题：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021183729.png?raw=true" alt="blog_images20221021183729.png"></p>
<p>注：F是假设空间<br>  当样本容量足够大时，经验风险最小化能保证有很好的学习效果。但是，当样本容量很小时，经验风险最小化学习的效果未必很好，会产生“过拟合”现象。<br>  结构风险最小化（structural risk minimization,SRM） 即为了防止过拟合而提出的策略。其通过在经验风险基础上加上表示模型复杂度的正则化项或惩罚项，在本质上等价于正则化。结构风险可定义如下：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021183729.png?raw=true" alt="blog_images20221021183729.png"></p>
<p>注：J(f)为模型的复杂度，是定义在假设空间F上的泛函<br>  结构风险最小化策略认为结构风险最小的模型是最优的模型，所以求最优模型就是求解最优化问题：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021183928.png?raw=true" alt="blog_images20221021183928.png"></p>
<p>     综上，监督学习问题就变成了经验风险或结构风险函数的最优化问题，经验风险或结构风险的函数就是最优化的目标函数。</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>算法指学习模型的具体计算方法。由于统计学问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。</p>
<h1 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h1><h2 id="训练误差与测试误差"><a href="#训练误差与测试误差" class="headerlink" title="训练误差与测试误差"></a>训练误差与测试误差</h2><p>  统计学习的目的是使学到的模型不仅对已知数据而且对未知数据都能有很好的预测能力。不同的学习方法会给出不同的模型，而基于损失函数的模型训练误差（training error） 和模型测试误差（test error） 就自然成为学习方法的评估的标准。<br>  训练误差的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要。测试误差反映了学习方法对未知的测试数据集的预测能力。通常将学习方法对未知数据的预测能力称为泛化能力。</p>
<h2 id="过拟合与模型选择"><a href="#过拟合与模型选择" class="headerlink" title="过拟合与模型选择"></a>过拟合与模型选择</h2><p>  当假设空间含有不同复杂度的模型时，就要面临模型选择的问题。如果在假设空间中存在“真”模型，那么所选择的模型应该逼近真模型。<br>  如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种现象称为过拟合（over-fitting）。这种现象表现为对已知数据预测得很好，但对为知数据预测得很差。</p>
<h1 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h1><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>  模型选择的典型方法是<strong>正则化（regularization）</strong>。正则化是结构风险最小化策略的实现，是<code>在经验风险上加一个正则化项或惩罚项</code>。一般具有如下形式：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021184148.png?raw=true" alt="blog_images20221021184148.png"></p>
<p>  利用正则化进行模型选择的方法符合奥卡姆剃刀原理，即在所有可能选择的模型中，<code>能够很好地解释已知数据并且十分简单</code>才是最好的模型，也就是应该选择的模型。</p>
<h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>​    另一种模型选择的典型方法是交叉验证（cross validation）。它的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择。</p>
<p>​    常见的交叉验证方法有以下三种：简单交叉验证、S折交叉验证、留一交叉验证</p>
<h1 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h1><p>​    学习方法的<strong>泛化能力（generalization ability）</strong> 是指由该方法学习到的模型<code>对未知数据的预测能力</code>，是学习方法本质上重要的性质。</p>
<h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><p>     监督学习方法又可以分为生成方法和判别方法，所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。<br>  生成方法由数据学习联合概率分布P ( X , Y ) ，然后求出条件概率分布P ( Y ∣ X ) 作为预测的模型（生成模型），之所以被称之为生成方法，是因为模型表示了给定输入X XX产生输出Y YY的生成关系。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">生成方法的特点：</span><br><span class="line">（1）生成方法可以还原出联合概率分布P ( X , Y ) ；</span><br><span class="line">（2）生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；</span><br><span class="line">（3）生成方法适用于存在隐变量的学习。</span><br></pre></td></tr></table></figure>

<p> 判别方法由数据直接学习决策函数f ( X ) 或者条件概率分布P ( Y ∣ X ) 作为预测模型（判别模型），判别方法关注于给定的输入X ，应该预测什么样的输出Y 。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">判别方法的特点：</span><br><span class="line">（1）判别方法直接学习条件概率P ( Y ∣ X ) 或决策函数f ( X ) ，直接面对预测，往往学习的准确率更高；</span><br><span class="line">（2）判别方法可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</span><br></pre></td></tr></table></figure>

<h1 id="监督学习应用"><a href="#监督学习应用" class="headerlink" title="监督学习应用"></a>监督学习应用</h1><p>​    监督学习的应用主要在三个方面：<strong>分类问题</strong>、<strong>标注问题</strong>和<strong>回归问题</strong>。</p>
<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p> 输入变量为有限个离散变量的预测问题称为分类问题。其表示如图</p>
<p><img src="https://img-blog.csdnimg.cn/65054a8e54ab43c3bb0d907e0e693756.png#pic_center" alt="img"></p>
<h2 id="标注问题"><a href="#标注问题" class="headerlink" title="标注问题"></a>标注问题</h2><p> 输入变量与输出变量均为变量序列的预测问题称为标注问题。其表示如图</p>
<p><img src="https://img-blog.csdnimg.cn/143fb3dc8f7d4574aaa88b2561f29e24.png#pic_center" alt="img"></p>
<h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>输入变量与输出变量均为连续变量的预测问题称为回归问题。其表示如图</p>
<p><img src="https://img-blog.csdnimg.cn/ae3d0da74c7d41cc9016f56894a49adf.png#pic_center" alt="img"></p>
<h1 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h1><h2 id="sklearn概述"><a href="#sklearn概述" class="headerlink" title="sklearn概述"></a>sklearn概述</h2><p>scikit-learn计划开始于scikits.learn，它是David Cournapeau（英语：David Cournapeau）的Google编程之夏计划。它的名字来源自成为“SciKit”（SciPy工具箱）的想法，即一个独立开发和发行的第三方SciPy扩展。最初的代码库被其他开发者重写了。在2010年，来自法国罗康库尔的法国国家信息与自动化研究所的Fabian Pedregosa、Gael Varoquaux、Alexandre Gramfort和Vincent Michel，领导了这个项目并在2010年2月1日进行了首次公开发行。在各种scikit中，scikit-learn和scikit-image（英语：scikit-image）截至2012年11月被称为“良好维护和流行的”。Scikit-learn是在GitHub上最流行的机器学习库之一。</p>
<h2 id="sklearn模块"><a href="#sklearn模块" class="headerlink" title="sklearn模块"></a>sklearn模块</h2><p><strong>注：sklearn里面只能接收二维的矩阵，如果只是一维请采用reshape(-1,1)</strong></p>
<p><strong>分类 (Classification)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> SomeClassifier	</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SomeClassifier	</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> SomeClassifier</span><br></pre></td></tr></table></figure>

<p><strong>回归 (Regression)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> SomeRegressor	</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SomeRegressor	</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> SomeRegressor</span><br></pre></td></tr></table></figure>

<p><strong>聚类 (Clustering)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p><strong>降维 (Dimensionality Reduction)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p><strong>模型选择 (Model Selection)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p><strong>预处理 (Preprocessing)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p>此外，Sklearn 里面还有很多自带数据集供，引入它们的伪代码如下。</p>
<p><strong>数据集 (Dataset)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> SomeData</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">datasets.load_boston <span class="comment">#波士顿房价数据集  回归模型</span></span><br><span class="line">datasets.load_breast_cancer <span class="comment">#乳腺癌数据集  分类模型</span></span><br><span class="line">datasets.load_diabetes <span class="comment">#糖尿病数据集  回归模型</span></span><br><span class="line">datasets.load_digits <span class="comment">#手写体数字数据集  分类模型</span></span><br><span class="line">datasets.load_files  </span><br><span class="line">datasets.load_iris <span class="comment">#鸢尾花数据集  分类模型</span></span><br><span class="line">datasets.load_lfw_pairs  </span><br><span class="line">datasets.load_lfw_people  </span><br><span class="line">datasets.load_linnerud <span class="comment">#体能训练数据集  回归模型</span></span><br><span class="line">datasets.load_mlcomp  </span><br><span class="line">datasets.load_sample_image  </span><br><span class="line">datasets.load_sample_images  </span><br><span class="line">datasets.load_svmlight_file  </span><br><span class="line">datasets.load_svmlight_files  </span><br></pre></td></tr></table></figure>

<h2 id="预处理-Preprocessing"><a href="#预处理-Preprocessing" class="headerlink" title="预处理(Preprocessing)"></a>预处理(Preprocessing)</h2><h3 id="数据无量纲化"><a href="#数据无量纲化" class="headerlink" title="数据无量纲化"></a>数据无量纲化</h3><p> 在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一规格，或不同分布的数据转换到某个特定分布 的需求，这种需求统称为将数据“无量纲化”。</p>
<p><strong>reprocessing.MinMaxScaler</strong></p>
<p>当数据data中的一个特征太大严重影响另外一个特征，数据data按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间，而这个过程，就叫做数据归一化(Normalization，又称Min-Max Scaling)。公式如下：<br>$$<br>x=\frac{x-min(x)}{max(x)-min(x)}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(data)</span><br><span class="line"><span class="comment">#实现归一化</span></span><br><span class="line">scaler = MinMaxScaler() <span class="comment">#实例化</span></span><br><span class="line">scaler = scaler.fit(data) <span class="comment">#fit，在这里本质是生成min(x)和max(x)</span></span><br><span class="line">result = scaler.transform(data) <span class="comment">#通过接口导出结果</span></span><br><span class="line">result</span><br><span class="line">result_ = scaler.fit_transform(data) <span class="comment">#训练和导出结果一步达成</span></span><br><span class="line">scaler.inverse_transform(result) <span class="comment">#将归一化后的结果逆转</span></span><br><span class="line"><span class="comment">#使用MinMaxScaler的参数feature_range实现将数据归一化到[0,1]以外的范围中</span></span><br><span class="line">data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line">scaler = MinMaxScaler(feature_range=[<span class="number">5</span>,<span class="number">10</span>]) <span class="comment">#依然实例化</span></span><br><span class="line">result = scaler.fit_transform(data) <span class="comment">#fit_transform一步导出结果</span></span><br><span class="line">result</span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.StandardScaler</strong></p>
<p> 当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分 布），而这个过程，就叫做数据标准化(Standardization）。公式如下<br>$$<br>x=\frac{x-\mu}{\sigma}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line">scaler = StandardScaler() <span class="comment">#实例化</span></span><br><span class="line">scaler.fit(data) <span class="comment">#fit，本质是生成均值和方差</span></span><br><span class="line">scaler.mean_ <span class="comment">#查看均值的属性mean_</span></span><br><span class="line">scaler.var_ <span class="comment">#查看方差的属性var_</span></span><br><span class="line">x_std = scaler.transform(data) <span class="comment">#通过接口导出结果</span></span><br><span class="line">x_std.mean() <span class="comment">#导出的结果是一个数组，用mean()查看均值</span></span><br><span class="line">x_std.std() <span class="comment">#用std()查看方差</span></span><br><span class="line">scaler.fit_transform(data) <span class="comment">#使用fit_transform(data)一步达成结果</span></span><br><span class="line">scaler.inverse_transform</span><br><span class="line">(x_std) <span class="comment">#使用inverse_transform逆转标准化</span></span><br></pre></td></tr></table></figure>

<p><strong>StandardScaler</strong>和<strong>MinMaxScaler</strong>选哪个？</p>
<p>​        看情况。大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。</p>
<p>​        MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。</p>
<p>​        建议先试试看StandardScaler，效果不好换MinMaxScaler。</p>
<p>​        除了StandardScaler和MinMaxScaler之外，sklearn中也提供了各种其他缩放处理（中心化只需要一个pandas广</p>
<p>播一下减去某个数就好了，因此sklearn不提供任何中心化功能）。比如，在希望压缩数据，却不影响数据的稀疏</p>
<p>性时（不影响矩阵中取值为0的个数时），我们会使用MaxAbsScaler；在异常值多，噪声非常大时，我们可能会选</p>
<p>用分位数来无量纲化，此时使用RobustScaler。更多详情请参考以下列表</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221022093634.png?raw=true" alt="blog_images20221022093634.png"></p>
<h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><p>机器学习和数据挖掘中所使用的数据，永远不可能是完美的。很多特征，对于分析和建模来说意义非凡，但对于实际收集数据的人却不是如此，因此数据挖掘之中，常常会有重要的字段缺失值很多，但又不能舍弃字段的情况。因此，数据预处理中非常重要的一项就是处理缺失值</p>
<p><strong>缺失值处理一般采用以下方式：<br>如果是数值类型，用平均值取代；<br>如果是分类数据，用最常见的类别取代；</strong></p>
<p><strong>impute.SimpleImputer</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=<span class="literal">None</span>, verbose=<span class="number">0</span>,</span><br><span class="line">copy=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>含义&amp;输入</strong></th>
</tr>
</thead>
<tbody><tr>
<td>missing_values</td>
<td>告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan</td>
</tr>
<tr>
<td>strategy</td>
<td>我们填补缺失值的策略，默认均值。输入“mean”使用均值填补（仅对数值型特征可用)                    输入“median”用中值填补（仅对数值型特征可用 ）                                                                          输入”most_frequent”用众数填补（对数值型和字符型特征都可用）                                                  输入“constant”表示请参考参数“fifill_value”中的值（对数值型和字符型特征都可用）</td>
</tr>
<tr>
<td>fifill_value</td>
<td>当参数startegy为”constant”的时候可用，可输入字符串或数字表示要填充的值，常用0</td>
</tr>
<tr>
<td>copy</td>
<td>默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去。</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Age = data.loc[:,<span class="string">&quot;Age&quot;</span>].values.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#sklearn当中特征矩阵必须是二维</span></span><br><span class="line">Age[:<span class="number">20</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line">imp_mean = SimpleImputer() <span class="comment">#实例化，默认均值填补</span></span><br><span class="line">imp_median = SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>) <span class="comment">#用中位数填补</span></span><br><span class="line">imp_0 = SimpleImputer(strategy=<span class="string">&quot;constant&quot;</span>,fill_value=<span class="number">0</span>) <span class="comment">#用0填补</span></span><br><span class="line">imp_mean = imp_mean.fit_transform(Age) <span class="comment">#fit_transform一步完成调取结果</span></span><br><span class="line">imp_median = imp_median.fit_transform(Age)</span><br><span class="line">imp_0 = imp_0.fit_transform(Age)</span><br></pre></td></tr></table></figure>

<p>补充:其实利用pandas和numpy进行填充更简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">r&quot;D:\Project\PythonProject\Python01\jupyter\train.csv&quot;</span>,index_col=<span class="number">0</span>)</span><br><span class="line">data.head()</span><br><span class="line">data.loc[:,<span class="string">&quot;Age&quot;</span>] = data.loc[:,<span class="string">&quot;Age&quot;</span>].fillna(data.loc[:,<span class="string">&quot;Age&quot;</span>].median())</span><br><span class="line"><span class="comment">#.fillna 在DataFrame里面直接进行填补</span></span><br><span class="line">data.dropna(axis=<span class="number">0</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#.dropna(axis=0)删除所有有缺失值的行，.dropna(axis=1)删除所有有缺失值的列</span></span><br><span class="line"><span class="comment">#参数inplace，为True表示在原数据集上进行修改，为False表示生成一个复制对象，不修改原数据，默认False</span></span><br></pre></td></tr></table></figure>

<h3 id="处理分类型特征：编码与哑变量"><a href="#处理分类型特征：编码与哑变量" class="headerlink" title="处理分类型特征：编码与哑变量"></a>处理分类型特征：编码与哑变量</h3><p>在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fifit的时候全部要求输入数组或矩阵，也不能够导入文字型数据（其实手写决策树和普斯贝叶斯可以处理文字，但是sklearn中规定必须导入数值型）。然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是[“小学”，“初中”，“高中”，”大学”]，付费方式可能包含[“支付宝”，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行编码，即是说，将文字型数据转换为数值型。</p>
<p><strong>preprocessing.LabelEncoder</strong>：标签专用，能够将分类转换为分类数值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">y = data.iloc[:,-<span class="number">1</span>] <span class="comment">#要输入的是标签，不是特征矩阵，所以允许一维</span></span><br><span class="line">le = LabelEncoder() <span class="comment">#实例化</span></span><br><span class="line">le = le.fit(y) <span class="comment">#导入数据</span></span><br><span class="line">label = le.transform(y)   <span class="comment">#transform接口调取结果</span></span><br><span class="line">le.classes_ <span class="comment">#属性.classes_查看标签中究竟有多少类别</span></span><br><span class="line">label <span class="comment">#查看获取的结果label</span></span><br><span class="line">le.fit_transform(y) <span class="comment">#也可以直接fit_transform一步到位</span></span><br><span class="line">le.inverse_transform(label) <span class="comment">#使用inverse_transform可以逆转</span></span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.OrdinalEncoder</strong>：特征专用，能够将分类特征转换为分类数值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"><span class="comment">#接口categories_对应LabelEncoder的接口classes_，一模一样的功能</span></span><br><span class="line">data_ = data.copy()</span><br><span class="line">data_.head()</span><br><span class="line">OrdinalEncoder().fit(data_.iloc[:,<span class="number">1</span>:-<span class="number">1</span>]).categories_</span><br><span class="line">data_.iloc[:,<span class="number">1</span>:-<span class="number">1</span>] = OrdinalEncoder().fit_transform(data_.iloc[:,<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">data_.head()</span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.OneHotEncoder</strong>：独热编码，创建哑变量</p>
<p>我们刚才已经用OrdinalEncoder把分类变量Sex和Embarked都转换成数字对应的类别了。在舱门Embarked这一列中，我们使用[0,1,2]代表了三个不同的舱门，然而这种转换是正确的吗？</p>
<p>我们来思考三种不同性质的分类数据：</p>
<p>1） 舱门（S，C，Q）</p>
<p>三种取值S，C，Q是相互独立的，彼此之间完全没有联系，表达的是S≠C≠Q的概念。这是名义变量。</p>
<p>2） 学历（小学，初中，高中）</p>
<p>三种取值不是完全独立的，我们可以明显看出，在性质上可以有高中&gt;初中&gt;小学这样的联系，学历有高低，但是学历取值之间却不是可以计算的，我们不能说小学 + 某个取值 = 初中。这是有序变量。</p>
<p>3） 体重（&gt;45kg，&gt;90kg，&gt;135kg）</p>
<p>各个取值之间有联系，且是可以互相计算的，比如120kg - 45kg = 90kg，分类之间可以通过数学计算互相转换。这是有距变量。然而在对特征进行编码的时候，这三种分类数据都会被我们转换为[0,1,2]，这三个数字在算法看来，是连续且可以计算的，这三个数字相互不等，有大小，并且有着可以相加相乘的联系。所以算法会把舱门，学历这样的分类特征，都误会成是体重这样的分类特征。这是说，我们把分类转换成数字的时候，忽略了数字中自带的数学性质，所以给算法传达了一些不准确的信息，而这会影响我们的建模。类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息。</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221022152130.png?raw=true" alt="blog_images20221022152130.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">&#x27;auto&#x27;</span>).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line">result</span><br><span class="line"><span class="comment">#依然可以直接一步到位，但为了给大家展示模型属性，所以还是写成了三步</span></span><br><span class="line">OneHotEncoder(categories=<span class="string">&#x27;auto&#x27;</span>).fit_transform(X).toarray()</span><br><span class="line"><span class="comment">#依然可以还原</span></span><br><span class="line">pd.DataFrame(enc.inverse_transform(result))</span><br><span class="line">enc.get_feature_names()</span><br><span class="line">result</span><br><span class="line">result.shape</span><br><span class="line"><span class="comment">#axis=1,表示跨行进行合并，也就是将量表左右相连，如果是axis=0，就是将量表上下相连</span></span><br><span class="line">newdata = pd.concat([data,pd.DataFrame(result)],axis=<span class="number">1</span>)</span><br><span class="line">newdata.head()</span><br><span class="line">newdata.drop([<span class="string">&quot;Sex&quot;</span>,<span class="string">&quot;Embarked&quot;</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">newdata.columns = [<span class="string">&quot;Age&quot;</span>,<span class="string">&quot;Survived&quot;</span>,<span class="string">&quot;Female&quot;</span>,<span class="string">&quot;Male&quot;</span>,<span class="string">&quot;Embarked_C&quot;</span>,<span class="string">&quot;Embarked_Q&quot;</span>,<span class="string">&quot;Embarked_S&quot;</span>]</span><br></pre></td></tr></table></figure>

<h3 id="处理连续型特征：二值化与分段"><a href="#处理连续型特征：二值化与分段" class="headerlink" title="处理连续型特征：二值化与分段"></a>处理连续型特征：二值化与分段</h3><p><strong>sklearn.preprocessing.Binarizer</strong></p>
<p>根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。二值化是对文本计数数据的常见操作，分析人员可以决定仅考虑某种现象的存在与否。它还可以用作考虑布尔随机变量的估计器的预处理步骤（例如，使用贝叶斯设置中的伯努利分布建模）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>].values.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#类为特征专用，所以不能使用一维数组</span></span><br><span class="line">transformer = Binarizer(threshold=<span class="number">30</span>).fit_transform(X)</span><br><span class="line">transformer</span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.KBinsDiscretizer</strong></p>
<p>这是将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。总共包含三个重要参数：</p>
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>含义&amp;输入</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>n_bins</strong></td>
<td>每个特征中分箱的个数，默认5，一次会被运用到所有导入的特征</td>
</tr>
<tr>
<td><strong>encode</strong></td>
<td>编码的方式，默认“onehot”                                                                                                                      “onehot”：做哑变量，之后返回一个稀疏矩阵，每一列是一个特征中的一个类别，含有该类别的样本表示为1，不含的表示为0                                                                                                                                                      “ordinal”：每个特征的每个箱都被编码为一个整数，返回每一列是一个特征，每个特征下含有不同整数编码的箱的矩阵                                                                                                                                                “onehot-dense”：做哑变量，之后返回一个密集数组。</td>
</tr>
<tr>
<td><strong>strategy</strong></td>
<td>用来定义箱宽的方式，默认”quantile”                                                                                                       “uniform”：表示等宽分箱，即每个特征中的每个箱的最大值之间的差为(特征.max() - 特征.min())/(n_bins)      “quantile”：表示等位分箱，即每个特征中的每个箱内的样本数量都相同                                                                    “kmeans”：表示按聚类分箱，每个箱中的值到最近的一维k均值聚类的簇心得距离都相同</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>].values.reshape(-<span class="number">1</span>,<span class="number">1</span>) </span><br><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">&#x27;ordinal&#x27;</span>, strategy=<span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line">est.fit_transform(X)</span><br><span class="line"><span class="comment">#查看转换后分的箱：变成了一列中的三箱</span></span><br><span class="line"><span class="built_in">set</span>(est.fit_transform(X).ravel())</span><br><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">&#x27;onehot&#x27;</span>, strategy=<span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line"><span class="comment">#查看转换后分的箱：变成了哑变量</span></span><br><span class="line">est.fit_transform(X).toarray()</span><br></pre></td></tr></table></figure>

<h3 id="Filter过滤法"><a href="#Filter过滤法" class="headerlink" title="Filter过滤法"></a>Filter过滤法</h3><p>过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法。它是根据各种统计检验中的分数以及相关性的各项指标来选择特征。</p>
<p><strong>方差过滤</strong></p>
<p> <strong>VarianceThreshold</strong></p>
<p>这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为<strong>0的特征。VarianceThreshold有重要参数threshold</strong>，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">selector = VarianceThreshold() <span class="comment">#实例化，不填参数默认方差为0</span></span><br><span class="line">X_var0 = selector.fit_transform(X) <span class="comment">#获取删除不合格特征之后的新特征矩阵</span></span><br><span class="line"><span class="comment">#也可以直接写成 X = VairanceThreshold().fit_transform(X)</span></span><br></pre></td></tr></table></figure>

<p>可以看见，我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，我们希望留下一半的特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数threshold的值输入就好了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)</span><br><span class="line">X.var().values</span><br><span class="line">np.median(X.var().values)</span><br><span class="line">X_fsvar.shape</span><br></pre></td></tr></table></figure>

<p>当特征是二分类时，特征的取值就是伯努利随机变量，这些变量的方差可以计算为：<br>$$<br>Var[x]=p(1-p)<br>$$<br>其中X是特征矩阵，p是二分类特征中的一类在这个特征中所占的概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征</span></span><br><span class="line">X_bvar = VarianceThreshold(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)).fit_transform(X)</span><br><span class="line">X_bvar.shape</span><br></pre></td></tr></table></figure>

<h3 id="相关性过滤"><a href="#相关性过滤" class="headerlink" title="相关性过滤"></a>相关性过滤</h3><p>方差挑选完毕之后，我们就要考虑下一个问题：相关性了。我们希望选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量信息。如果特征与标签无关，那只会白白浪费我们的计算内存，可能还会给模型带来噪音。在sklearn当中，我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息</p>
<p><strong>卡方过滤</strong></p>
<p>卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类<strong>feature_selection.chi2</strong>计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合<strong>feature_selection.SelectKBest</strong>这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。另外，如果卡方检验检测到某个特征中所有的值都相同，会提示我们使用方差先进行方差过滤。并且，刚才我们已经验证过，当我们使用方差过滤筛选掉一半的特征后，模型的表现时提升的。因此在这里，我们使用threshold=中位数时完成的方差过滤的数据来做卡方检验（如果方差过滤后模型的表现反而降低了，那我们就不会使用方差过滤后的数据，而是使用原数据）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="comment">#假设在这里我一直我需要300个特征</span></span><br><span class="line">X_fschi = SelectKBest(chi2, k=<span class="number">300</span>).fit_transform(X_fsvar, y)</span><br><span class="line">X_fschi.shape</span><br></pre></td></tr></table></figure>

<p>验证一下模型的效果如何：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<p>如果效果降低了那么说明k=300的时候删除了模型相关的有效特征。那如何设置一个最佳的k值呢？在现实数据中，数据量很大，模型很复杂的时候，我们也许不能先去跑一遍模型看。看效果，而是希望最开始就能够选择一个最优的超参数k。那第一个方法，就是我们之前提过的学习曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#======【TIME WARNING: 5 mins】======#</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">390</span>,<span class="number">200</span>,-<span class="number">10</span>):</span><br><span class="line">    X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">350</span>,<span class="number">200</span>,-<span class="number">10</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>通过图例扎到最好的效果</p>
<table>
<thead>
<tr>
<th>p值</th>
<th>&lt;=0.05或0.01</th>
<th>&gt;0.05或0.01</th>
</tr>
</thead>
<tbody><tr>
<td>数据差异</td>
<td>差异不是自然形成的</td>
<td>这些差异是很自然的样本误差</td>
</tr>
<tr>
<td>相关性</td>
<td>两组数据是相关的</td>
<td>两组数据是相互独立的</td>
</tr>
<tr>
<td>原假设</td>
<td>拒绝原假设，接受备择假设</td>
<td>接受原假设</td>
</tr>
</tbody></table>
<p>从特征工程的角度，我们希望选取卡方值很大，p值小于0.05的特征，即和标签是相关联的特征。而调用SelectKBest之前，我们可以直接从chi2实例化后的模型中获得各个特征所对应的卡方值和P值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chivalue, pvalues_chi = chi2(X_fsvar,y)</span><br><span class="line"><span class="comment">#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征：</span></span><br><span class="line">k = chivalue.shape[<span class="number">0</span>] - (pvalues_chi &gt; <span class="number">0.05</span>).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">#X_fschi = SelectKBest(chi2, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<p> <strong>F检验</strong></p>
<p>F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而F检验回归用于标签是连续型变量的数据。和卡方检验一样，这两个类需要和类SelectKBest连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的K。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式。F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，我们希望选取<strong>p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的</strong>，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。以F检验的分类为例，我们继续在数字数据集上来进行特征选择</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line">F, pvalues_f = f_classif(X_fsvar,y)</span><br><span class="line">k = F.shape[<span class="number">0</span>] - (pvalues_f &gt; <span class="number">0.05</span>).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">#X_fsF = SelectKBest(f_classif, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsF,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<p>得到的结论和我们用卡方过滤得到的结论一模一样：没有任何特征的p值大于0.01，所有的特征都是和标签相关的，因此我们不需要相关性过滤。</p>
<p><strong>互信息法</strong></p>
<p>互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和feature_selection.mutual_info_regression（互信息回归）。这两个类的用法和参数都和F检验一模一样 ，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系。互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。以互信息分类为例的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_classif <span class="keyword">as</span> MIC</span><br><span class="line">result = MIC(X_fsvar,y) <span class="comment">#X_fsvar为特征,y为标签</span></span><br><span class="line">k = result.shape[<span class="number">0</span>] - <span class="built_in">sum</span>(result &lt;= <span class="number">0</span>)</span><br><span class="line"><span class="comment">#X_fsmic = SelectKBest(MIC, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsmic,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<p>所有特征的互信息量估计都大于0，因此所有特征都与标签相关。当然了，无论是F检验还是互信息法，大家也都可以使用学习曲线，只是使用统计量的方法会更加高效。当统计量判断已经没有特征可以删除时，无论用学习曲线如何跑，删除特征都只会降低模型的表现。当然了，如果数据量太庞大，模型太复杂，我们还是可以牺牲模型表现来提升模型速度，一切都看大家的具体需求。</p>
<h3 id="Embedded嵌入法"><a href="#Embedded嵌入法" class="headerlink" title="Embedded嵌入法"></a>Embedded嵌入法</h3><p>嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性，比如决策树和树的集成模型中的feature_importances_属性，可以列出各个特征对树的建立的贡献，我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。因此相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.feature_selection.SelectFromModel (estimator, threshold=<span class="literal">None</span>, prefit=<span class="literal">False</span>, norm_order=<span class="number">1</span>,</span><br><span class="line">max_features=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>参数 说明</p>
<p><strong>estimator</strong> </p>
<p>使用的模型评估器，只要是带feature_importances_或者coef_属性，或带有l1和l2惩罚项的模型都可以使用</p>
<p><strong>threshold</strong> </p>
<p>特征重要性的阈值，重要性低于这个阈值的特征都将被删除</p>
<p><strong>prefifit</strong></p>
<p>默认False，判断是否将实例化后的模型直接传递给构造函数。如果为True，则必须直接调用fifit和transform，不能使用fifit_transform，并且SelectFromModel不能与cross_val_score，GridSearchCV和克隆估计器的类似实用程序一起使用。</p>
<p><strong>norm_order</strong></p>
<p>k可输入非零整数，正无穷，负无穷，默认值为1 ,在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数。</p>
<p><strong>max_features</strong> </p>
<p>在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据max_features选择，请设置threshold = -np.inf</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">X_embedded = SelectFromModel(RFC_,threshold=<span class="number">0.005</span>).fit_transform(X,y)</span><br><span class="line"><span class="comment">#在这里我只想取出来有限的特征。0.005这个阈值对于有780个特征的数据来说，是非常高的阈值，因为平均每个特征只能够分到大约0.001的feature_importances_</span></span><br></pre></td></tr></table></figure>

<h3 id="Wrapper包装法"><a href="#Wrapper包装法" class="headerlink" title="Wrapper包装法"></a>Wrapper包装法</h3><p>包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如coef_属性或feature_importances_属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。区别于过滤法和嵌入法的一次训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.feature_selection.RFE (estimator, n_features_to_select=<span class="literal">None</span>, step=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>参数<strong>estimator</strong>是需要填写的实例化后的评估器，<strong>n_features_to_select</strong>是想要选择的特征个数，<strong>step</strong>表示每次迭代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，**.support_<strong>：返回所有的特征的是否最后被选中的布尔矩阵，以及</strong>.ranking_**返回特征的按数次迭代中综合重要性的排名。类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">selector = RFE(RFC_, n_features_to_select=<span class="number">340</span>, step=<span class="number">50</span>).fit(X, y)</span><br></pre></td></tr></table></figure>



<h2 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h2><p><strong>算法</strong></p>
<p>遍历所有的样本点，计算每个样本点与待分类数据的距离，找出k个距离最近的点，统计每个类别的个数，投票数据最多的类别即为样本点的类别。</p>
<p>sklearn提供的模块</p>
<table>
<thead>
<tr>
<th>类方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>KNeighborsClassifier</td>
<td>KNN 算法解决分类问题</td>
</tr>
<tr>
<td>KNeighborsRegressor</td>
<td>KNN 算法解决回归问题</td>
</tr>
<tr>
<td>RadiusNeighborsClassifier</td>
<td>基于半径来查找最近邻的分类算法</td>
</tr>
<tr>
<td>NearestNeighbors</td>
<td>基于无监督学习实现KNN算法</td>
</tr>
<tr>
<td>KDTree</td>
<td>无监督学习下基于 KDTree 来查找最近邻的分类算法</td>
</tr>
<tr>
<td>BallTree</td>
<td>无监督学习下基于 BallTree 来查找最近邻的分类算法</td>
</tr>
</tbody></table>
<p><strong>sklearn.neighbors.KNeighborsClassifier参数说明</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier ( n_neighbors = <span class="number">5</span> , * , weights = <span class="string">&#x27;uniform&#x27;</span> , algorithm = <span class="string">&#x27;auto&#x27;</span> , leaf_size = <span class="number">30</span> , p = <span class="number">2</span> , metric = <span class="string">&#x27;minkowski&#x27;</span> , metric_params = <span class="literal">None</span> , n_jobs = <span class="literal">None</span> )</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>n_neighbors int，默认=5</strong></p>
<p>默认情况下用于kneighbors查询的邻居数。</p>
<p><strong>weights{‘uniform’, ‘distance’} ，默认=’uniform’</strong></p>
<p>预测中使用的权重函数。可能的值：</p>
<ul>
<li>‘uniform’ ：统一的权重。每个邻域中的所有点的权重相等。</li>
<li>‘distance’ ：权重点的距离的倒数。在这种情况下，查询点的较近的邻居将比较远的邻居具有更大的影响。</li>
<li>[callable] ：一个用户定义的函数，它接受一个距离数组，并返回一个包含权重的相同形状的数组。</li>
</ul>
<p><strong>algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’</strong></p>
<p>用于计算最近邻的算法：</p>
<ul>
<li>‘ball_tree’ 将使用BallTree</li>
<li>‘kd_tree’ 将使用KDTree</li>
<li>‘brute’ 将使用暴力搜索。</li>
<li>‘auto’ 将尝试根据传递给fit方法的值来决定最合适的算法。</li>
</ul>
<p>注意：拟合稀疏输入将使用蛮力覆盖此参数的设置。</p>
<p><strong>Leaf_size int，默认=30</strong></p>
<p>叶大小传递给 BallTree 或 KDTree。这会影响构建和查询的速度，以及存储树所需的内存。最佳值取决于问题的性质。</p>
<p><strong>p int，默认=2</strong></p>
<p>Minkowski 度量的功率参数。当 p = 1 时，这相当于使用 manhattan_distance (l1)，而 p = 2 则使用 euclidean_distance (l2)。对于任意 p，使用 minkowski_distance (l_p)。</p>
<p><strong>metric ：str 或可调用，默认=’minkowski’</strong></p>
<p>用于距离计算的度量。默认为“minkowski”，当 p = 2 时产生标准欧几里德距离。请参阅scipy.spatial.distance的文档和列出 distance_metrics的有效度量值的度量。</p>
<p>如果 metric 是“预先计算的”，则 X 被假定为一个距离矩阵，并且在拟合期间必须是平方的。X 可能是一个稀疏图，在这种情况下，只有“非零”元素可以被认为是邻居。</p>
<p>如果 metric 是一个可调用函数，它需要两个表示一维向量的数组作为输入，并且必须返回一个值来指示这些向量之间的距离。这适用于 Scipy 的指标，但效率低于将指标名称作为字符串传递。</p>
<p><strong>metric_params:dic，默认=无</strong></p>
<p>度量函数的附加关键字参数。</p>
<p><strong>n_jobs :int</strong>，默认=无</p>
</blockquote>
<p>属性：</p>
<blockquote>
<p><strong>Effective_metric_str 或callble</strong></p>
<p>使用的距离度量。它将与<code>metric</code>参数或其同义词相同，例如，如果<code>metric</code>参数设置为“minkowski”且<code>p</code>参数设置为 2，则为“euclidean”。</p>
<p>*<strong>Effective_metric_params_dict _：</strong></p>
<p>度量函数的附加关键字参数。对于大多数指标将与参数相同，但如果属性设置为“minkowski” <code>metric_params</code>，也可能包含 <code>p</code>参数值。<code>effective_metric_</code></p>
<p><strong>n_features_in_ int</strong></p>
<p>拟合期间看到的特征数。</p>
<p><em>0.24 版中的新功能。</em></p>
<p><strong>feature_names_in_ndarray 的形状 ( <code>n_features_in_</code>,)</strong></p>
<p>拟合期间看到的特征名称。仅当<code>X</code> 具有全为字符串的特征名称时才定义。</p>
<p><em>1.0 版中的新功能。</em></p>
<p><strong>n_samples_fit_int_</strong></p>
<p>拟合数据中的样本数。</p>
<p><strong>outputs_2d_bool</strong></p>
<p>当<code>y</code>‘ 的形状在拟合期间为 (n_samples, ) 或 (n_samples, 1) 时为假，否则为真。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载红酒数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line"><span class="comment">#KNN分类算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment">#分割训练集与测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment">#导入numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#加载数据集</span></span><br><span class="line">wine_dataset=load_wine()</span><br><span class="line"><span class="comment">#查看数据集对应的键</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;红酒数据集的键:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(wine_dataset.keys()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集描述:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(wine_dataset[<span class="string">&#x27;data&#x27;</span>].shape))</span><br><span class="line"><span class="comment"># data 为数据集数据;target 为样本标签</span></span><br><span class="line"><span class="comment">#分割数据集，比例为 训练集：测试集 = 8:2</span></span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(wine_dataset[<span class="string">&#x27;data&#x27;</span>],wine_dataset[<span class="string">&#x27;target&#x27;</span>],test_size=<span class="number">0.2</span>,random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#构建knn分类模型，并指定 k 值</span></span><br><span class="line">KNN=KNeighborsClassifier(n_neighbors=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#使用训练集训练模型</span></span><br><span class="line">KNN.fit(X_train,y_train)</span><br><span class="line"><span class="comment">#评估模型的得分</span></span><br><span class="line">score=KNN.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="comment">#给出一组数据对酒进行分类</span></span><br><span class="line">X_wine_test=np.array([[<span class="number">11.8</span>,<span class="number">4.39</span>,<span class="number">2.39</span>,<span class="number">29</span>,<span class="number">82</span>,<span class="number">2.86</span>,<span class="number">3.53</span>,<span class="number">0.21</span>,<span class="number">2.85</span>,<span class="number">2.8</span>,<span class="number">.75</span>,<span class="number">3.78</span>,<span class="number">490</span>]])</span><br><span class="line">predict_result=KNN.predict(X_wine_test)</span><br><span class="line"><span class="built_in">print</span>(predict_result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分类结果：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(wine_dataset[<span class="string">&#x27;target_names&#x27;</span>][predict_result]))</span><br></pre></td></tr></table></figure>



<h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>信息熵</strong>：决策树学习的关键是如何选择最优划分属性。划分过程中，决策树的分支结点所包含的样本尽可能属于同一类别，结点的“纯度”（purity）越来越高。信息熵是度量样本集合纯度最常用的一种指标。假定当前样本集合D DD中第k kk类样本所占的比例来为Pk (k=1,2,…,|y|)，则D的信息熵定义为</p>
<p>函数原型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.tree.DecisionTreeClassifier(*, criterion=<span class="string">&#x27;gini&#x27;</span>, splitter=<span class="string">&#x27;best&#x27;</span>, max_depth=<span class="literal">None</span>, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, class_weight=<span class="literal">None</span>, ccp_alpha=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>criterion ： gini或者entropy，前者是基尼指数，后者是信息熵；</li>
<li>max_depth ： int or None, optional (default=None) 设置决策随机森林中的决策树的最大深度，深度越大，越容易过合，推荐树的深度为：5-20之间；</li>
<li>max_features： None（所有），log2，sqrt，N 特征小于50的时候一般使用所有的；</li>
<li>max_leaf_nodes ： 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pandas用于处理和分析数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="comment"># 导入决策树分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="comment"># # 导入分割数据集的方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># import relevant packages</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_text</span><br><span class="line"><span class="comment"># import matplotlib; matplotlib.use(&#x27;TkAgg&#x27;)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用决策树进行鸢尾花数据集分类预测</span></span><br><span class="line"><span class="comment"># 数据集字段说明：</span></span><br><span class="line"><span class="comment"># 特征值（4个）：sepal length（花萼长度），sepal width（花萼宽度）， petal length（花瓣长度），petal width（花瓣宽度）</span></span><br><span class="line"><span class="comment"># 目标值（3个）：target（类别，0为&#x27;setosa&#x27;山鸢尾花，1为&#x27;versicolor&#x27;变色鸢尾花，2为&#x27;virginica&#x27;维吉尼亚鸢尾花）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load in the data加载数据</span></span><br><span class="line">data = load_iris()</span><br><span class="line"><span class="comment"># convert to a dataframe 转换数据格式</span></span><br><span class="line">df = pd.DataFrame(data.data, columns = data.feature_names)</span><br><span class="line"><span class="comment"># create the species column</span></span><br><span class="line">df[<span class="string">&#x27;Species&#x27;</span>] = data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># replace this with the actual names</span></span><br><span class="line">target = np.unique(data.target)  <span class="comment"># 对于一维数组或者列表，unique函数去除其中重复的元素，并按元素由大到小返回一个新的无元素重复的元组或者列表</span></span><br><span class="line">target_names = np.unique(data.target_names)</span><br><span class="line">targets = <span class="built_in">dict</span>(<span class="built_in">zip</span>(target, target_names))</span><br><span class="line">df[<span class="string">&#x27;Species&#x27;</span>] = df[<span class="string">&#x27;Species&#x27;</span>].replace(targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract features and target variables 提取特征和目标变量</span></span><br><span class="line">x = df.drop(columns=<span class="string">&quot;Species&quot;</span>)</span><br><span class="line">y = df[<span class="string">&quot;Species&quot;</span>]</span><br><span class="line"><span class="comment"># save the feature name and target variables 保存特征名称和目标变量</span></span><br><span class="line">feature_names = x.columns</span><br><span class="line">labels = y.unique()  <span class="comment"># 去除重复元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割训练集、测试集</span></span><br><span class="line"><span class="comment"># x 数据集的特征值</span></span><br><span class="line"><span class="comment"># y 数据集的标签值</span></span><br><span class="line"><span class="comment"># 训练集的特征值x_train 测试集的特征值x_test(test_x) 训练集的目标值y_train 测试集的目标值y_test(test_lab)</span></span><br><span class="line"><span class="comment"># random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</span></span><br><span class="line">X_train, test_x, y_train, test_lab = train_test_split(x,y,</span><br><span class="line">                                                 test_size = <span class="number">0.4</span>,</span><br><span class="line">                                                 random_state = <span class="number">42</span>)</span><br><span class="line"><span class="comment"># 创建决策树分类器（树的最大深度为3）</span></span><br><span class="line">model = DecisionTreeClassifier(max_depth =<span class="number">3</span>, random_state = <span class="number">42</span>)  <span class="comment"># 初始化模型</span></span><br><span class="line">model.fit(X_train, y_train)  <span class="comment"># 训练模型</span></span><br><span class="line"><span class="built_in">print</span>(model.score(test_x,test_lab))  <span class="comment"># 评估模型分数</span></span><br><span class="line"><span class="comment"># 计算每个特征的重要程度</span></span><br><span class="line"><span class="built_in">print</span>(model.feature_importances_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化特征属性结果</span></span><br><span class="line">r = export_text(model, feature_names=data[<span class="string">&#x27;feature_names&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt the figure, setting a black background</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>,<span class="number">10</span>), facecolor =<span class="string">&#x27;g&#x27;</span>)  <span class="comment"># facecolor设置背景色</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create the tree plot 决策树绘图模块，实现决策树可视化</span></span><br><span class="line">a = tree.plot_tree(model,</span><br><span class="line">                   <span class="comment"># use the feature names stored</span></span><br><span class="line">                   feature_names = feature_names,</span><br><span class="line">                   <span class="comment"># use the class names stored</span></span><br><span class="line">                   class_names = labels,</span><br><span class="line">                   <span class="comment"># label=&#x27;all&#x27;,</span></span><br><span class="line">                   rounded = <span class="literal">True</span>,</span><br><span class="line">                   filled = <span class="literal">True</span>,</span><br><span class="line">                   fontsize=<span class="number">14</span>,</span><br><span class="line">                   )</span><br><span class="line"><span class="comment"># show the plot</span></span><br><span class="line"><span class="comment"># plt.legend(loc=&#x27;lower right&#x27;, borderpad=0, handletextpad=0)</span></span><br><span class="line">plt.savefig(<span class="string">&quot;save.png&quot;</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">&quot;tight&quot;</span>)</span><br><span class="line"><span class="comment"># plt.tight_layout()</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p><strong>感知机简介</strong></p>
<blockquote>
<p>感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面。感知机是一种线性分类模型。</p>
</blockquote>
<p>感知机实际上表示为输入空间到输出空间的映射函数，如下所示：</p>
<p>$$<br>f(x)=sign(w.x+b)<br>$$<br>其中，<img src="https://www.zhihu.com/equation?tex=w" alt="w">和<img src="https://www.zhihu.com/equation?tex=b" alt="b">称为感知机的模型参数，<img src="https://www.zhihu.com/equation?tex=w%5Csubset+R%5E%7Bn%7D+" alt="w\subset R^{n}">叫做权值（weight）或权值向量（weight vector），<img src="https://www.zhihu.com/equation?tex=b%5Csubset+R" alt="b\subset R">叫做偏置（bias），<img src="https://www.zhihu.com/equation?tex=w%5Cbullet+x" alt="w\bullet x">是<img src="https://www.zhihu.com/equation?tex=w" alt="w">和<img src="https://www.zhihu.com/equation?tex=x" alt="x">的内积，<img src="https://www.zhihu.com/equation?tex=sign" alt="sign">是符号函数，其定义形式如下：</p>
<p>$$<br>sign=<br>\begin{cases}<br>-1,\quad x\leq 0\<br>1, \quad x&gt;0<br>\end{cases}<br>\tag{1}<br>$$</p>
<p><strong>感知机只能做二分类问题</strong></p>
<p><strong>sklearn实现感知机</strong></p>
<p>生成数据集：生成1000个样本，每个样本4个特征，输出有2个类别，没有冗余特征，每个类别一个簇</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line">x,y=make_classification(n_samples=<span class="number">1000</span>,n_features=<span class="number">4</span>,n_redundant=<span class="number">0</span>,n_informative=<span class="number">1</span>,n_clusters_per_class=<span class="number">1</span>,n_classes=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape,y.shape)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">(<span class="number">1000</span>, <span class="number">4</span>) (<span class="number">1000</span>,)</span><br></pre></td></tr></table></figure>

<p>数据预处理：划分数据集并标准化数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y)</span><br><span class="line">std = StandardScaler()</span><br><span class="line">x_train = std.fit_transform(x_train)P</span><br><span class="line">x_test = std.fit_transform(x_test)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape,x_test.shape)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">(<span class="number">750</span>, <span class="number">4</span>) (<span class="number">250</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>训练和评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">model = Perceptron()</span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line">y_pred = model.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(accuracy_score(y_pred,y_test))</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="number">0.996</span></span><br></pre></td></tr></table></figure>

<h2 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h2><p>在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(x),要么是决策条件分布P(Y|X)，但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布，然后由P(Y|X)=P(X,Y)/P(X)得出分类的结果。</p>
<p>如果想了解朴素贝叶斯算法的可以移步</p>
<p><strong>sklearn中的贝叶斯分类器</strong></p>
<p>Sklearn基于数据分布以及这些分布上的概率估计的改进，为我们提供了四个朴素贝叶斯的分类器。</p>
<table>
<thead>
<tr>
<th><strong>类</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td>naive_bayes.BernoulliNB</td>
<td>伯努利分布下的朴素贝叶斯</td>
</tr>
<tr>
<td>naive_bayes.GaussianNB</td>
<td>高斯分布下的朴素贝叶斯</td>
</tr>
<tr>
<td>naive_bayes.MultinomialNB</td>
<td>多项式分布下的朴素贝叶斯</td>
</tr>
<tr>
<td>naive_bayes.ComplementNB</td>
<td>补集朴素贝叶斯</td>
</tr>
<tr>
<td>linear_model.BayesianRidge</td>
<td>贝叶斯岭回归，在参数估计过程中使用贝叶斯回归技术来包括正则化参数</td>
</tr>
</tbody></table>
<p>虽然朴素贝叶斯使用了过于简化的假设，这个分类器在许多实际情况中都运行良好，著名的是文档分类和垃圾邮件过滤。而且由于贝叶斯是从概率角度进行估计，它所需要的样本量比较少，极端情况下甚至可以使用1%的数据作为训练集，依然可以得到很好的拟合效果。当然，如果样本量少于特征数目，贝叶斯效果就会被削弱（这对于任何算法都是一样的）。<br>与SVM和随机森林相比，朴素贝叶斯运行速度更快，因为求解P ( X i ∣ Y )本质是在每个特征上单独对概率进行计算，然后再求乘积，所以每个特征上的计算可以是独立并且并行的，因此贝叶斯的计算速度比较快。不过相对的，贝叶斯的运行效果不是那么好，所以贝叶斯的接口调用的predict_proba其实也不是总指向真正的分类结果，这一点需要注意。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%%cmd</span></span><br><span class="line"><span class="comment">#pip install watermark</span></span><br><span class="line"><span class="comment">#魔法命令必须是一个cell的第一部分内容</span></span><br><span class="line"><span class="comment">#注意load_ext这个命令只能够执行一次，再执行就会报错，要求用reload命令</span></span><br><span class="line">%load_ext watermark</span><br><span class="line">%watermark -a <span class="string">&quot;86188&quot;</span> -d -v -m -p numpy,pandas,matplotlib,scipy,sklearn</span><br><span class="line"><span class="comment">#导入需要的库和数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line">x,y = digits.data, digits.target</span><br><span class="line">xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size = <span class="number">0.3</span>,random_state = <span class="number">420</span>)</span><br><span class="line">xtrain.shape<span class="comment">#64个特征</span></span><br><span class="line"><span class="comment">#结果：(1257, 64)</span></span><br><span class="line">xtest.shape</span><br><span class="line"><span class="comment">#结果：(540, 64)</span></span><br><span class="line">np.unique(ytrain)<span class="comment">#多分类问题，类别是10个</span></span><br><span class="line"><span class="comment">#结果：array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#建模，探索建模结果</span></span><br><span class="line">gnb = GaussianNB().fit(xtrain,ytrain)</span><br><span class="line"><span class="comment">#查看分数</span></span><br><span class="line">acc_score = gnb.score(xtest,ytest)</span><br><span class="line">acc_score</span><br><span class="line"><span class="comment">#结果：0.8592592592592593</span></span><br><span class="line"><span class="comment">#查看预测结果</span></span><br><span class="line">y_pred = gnb.predict(xtest)</span><br><span class="line"><span class="comment">#查看预测的概率结果</span></span><br><span class="line">prob = gnb.predict_proba(xtest)</span><br><span class="line">prob.shape<span class="comment">#每一列对应一个标签类别下的概率</span></span><br><span class="line"><span class="comment">#结果：(540, 10)</span></span><br><span class="line">prob[<span class="number">1</span>:].<span class="built_in">sum</span>()<span class="comment">#每一行的和都是1</span></span><br><span class="line"><span class="comment">#结果：1.000000000000003</span></span><br><span class="line">prob.<span class="built_in">sum</span>(axis=<span class="number">1</span>).shape</span><br><span class="line"><span class="comment">#结果：(540,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用混淆矩阵来查看贝叶斯的分类结果</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM</span><br><span class="line">CM(ytest,y_pred)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">结果：</span></span><br><span class="line"><span class="string">array([[47,  0,  0,  0,  0,  0,  0,  1,  0,  0],</span></span><br><span class="line"><span class="string">       [ 0, 46,  2,  0,  0,  0,  0,  3,  6,  2],</span></span><br><span class="line"><span class="string">       [ 0,  2, 35,  0,  0,  0,  1,  0, 16,  0],</span></span><br><span class="line"><span class="string">       [ 0,  0,  1, 40,  0,  1,  0,  3,  4,  0],</span></span><br><span class="line"><span class="string">       [ 0,  0,  1,  0, 39,  0,  1,  4,  0,  0],</span></span><br><span class="line"><span class="string">       [ 0,  0,  0,  2,  0, 58,  1,  1,  1,  0],</span></span><br><span class="line"><span class="string">       [ 0,  0,  1,  0,  0,  1, 49,  0,  0,  0],</span></span><br><span class="line"><span class="string">       [ 0,  0,  0,  0,  0,  0,  0, 54,  0,  0],</span></span><br><span class="line"><span class="string">       [ 0,  3,  0,  1,  0,  0,  0,  2, 55,  0],</span></span><br><span class="line"><span class="string">       [ 1,  1,  0,  1,  2,  0,  0,  3,  7, 41]], dtype=int64)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#注意，ROC曲线是不能用于多分类的。多分类状况下最佳的模型评估指标是混淆矩阵和整体的准确度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h2><p>逻辑回归（logistic regression）属于机器学习中的监督算法.</p>
<p>虽然名字中带有“回归”二字，但却干着分类的活~这主要是由于sigmod和softmax函数的特点，可以将任意实数映射到（0，1）区间，这样我们就可以得到一个概率值，根据概率的大小，再结合定下的阈值，就可以进行分类。</p>
<p>逻辑回归相关的类：</p>
<blockquote>
<p>linear_model.LogisticRegression：逻辑回归分类器（又叫logit，最大熵分类器）<br>linear_model.LogisticRegressionCV：带交叉验证的逻辑回归分类器<br>linear_model.logistic_regression_path：计算Logistic回归模型以获得正则化参数的列表<br>linear_model.SGDClasiifier：利用梯度下降求解的线性分类器（SVM，逻辑回归等）<br>linear_model.SGDRegressor：利用梯度下降最小化正则化后的损失函数的线性回归模型<br>metrics.log_loss：对数损失，又称逻辑损失或交叉熵损失</p>
</blockquote>
<p>注：linear_model.RandomizedLogisticsRegression（随机的逻辑回归）在sklearn0.21版本中即将被移除</p>
<p><strong>其他会涉及的类</strong>：</p>
<blockquote>
<p>metrics.confusion_matrix：混淆矩阵，模型评估指标之一<br>metrics.roc_auc_score：ROC曲线，模型评估指标之一<br>metrics.accuracy_score：精确性，模型评估指标之一</p>
</blockquote>
<p>导包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris   <span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split   <span class="comment"># 导入数据划分函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression   <span class="comment"># 导入逻辑回归</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 导入评价指标</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score   </span><br></pre></td></tr></table></figure>

<p>属于处理</p>
<p>鸢尾花的label原本是3类，这里为了展示二分类，我只取了鸢尾花的前100个数据，也就是label只有0和1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line"> </span><br><span class="line">iris_X = iris.data[:<span class="number">100</span>, ]   <span class="comment"># x有4个属性，共有100个样本</span></span><br><span class="line">iris_X</span><br><span class="line"></span><br><span class="line">iris_y = iris.target[:<span class="number">100</span>, ] <span class="comment"># y的取值有2个，分别是0,1</span></span><br><span class="line">iris_y</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=<span class="number">0.3</span>)   <span class="comment"># 划分训练/测试集</span></span><br><span class="line"><span class="built_in">print</span>(X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(X_test.shape)</span><br></pre></td></tr></table></figure>

<p>训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = LogisticRegression()   <span class="comment"># 选逻辑回归作为分类器</span></span><br><span class="line">model.fit(X_train, y_train)   <span class="comment"># 训练模型</span></span><br></pre></td></tr></table></figure>

<p>评估模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">y_test_pred = model.predict(X=X_test)   <span class="comment"># 预测测试集的label</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(y_test_pred)   <span class="comment"># 模型预测的测试集label</span></span><br><span class="line"><span class="built_in">print</span>(y_test)   <span class="comment"># 测试集实际label </span></span><br><span class="line"></span><br><span class="line">accuracy_score(y_test, y_test_pred) <span class="comment"># 查看模型预测的准确率</span></span><br><span class="line"></span><br><span class="line">pred = model.predict_proba(X_test)   <span class="comment"># 查看测试集样本对应的概率值</span></span><br><span class="line">pred</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.coef_)   <span class="comment"># 查看变量参数</span></span><br><span class="line"><span class="built_in">print</span>(model.intercept_)   <span class="comment"># 查看常数参数项</span></span><br></pre></td></tr></table></figure>

<h2 id="SVM算法"><a href="#SVM算法" class="headerlink" title="SVM算法"></a>SVM算法</h2><blockquote>
<p>  支持向量机(support vector machines,SVM)是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。除此之外，SVM算法还包括核函数，核函数可以使它成为非线性分类器。在了解SVM算法之前，我们要先认识一下线性分类器。</p>
<p>线性分类器：假设在一个二维线性可分的数据集中，我们要找到一个超平面把两组数据分开，已知的方法有我们已经学过的线性回归和逻辑回归，这条直线可以有很多种，如下图的H1、H2、H3哪一条直线的效果最好呢，也就是说哪条直线可以使两类的空间大小相隔最大呢？</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/img_convert/68cf3d04a294b7f304c44d8a1a33ba3e.png" alt="img"></p>
<p>   我们凭直观感受应该觉得答案是H3。首先H1不能把类别分开，这个分类器肯定是不行的；H2可以，但分割线与最近的数据点只有很小的间隔，如果测试数据有一些噪声的话可能就会被H2错误分类(即对噪声敏感、泛化能力弱)。H3以较大间隔将它们分开，这样就能容忍测试数据的一些噪声而正确分类，是一个泛化能力不错的分类器。因此我们把这个划分数据的决策边界就叫做超平面。离这个超平面最近的点就是”支持向量”,点到超平面的距离叫做间隔，支持向量机的意思就是使超平面和支持向量之间的间隔尽可能的大，这样才可以使两类样本准确地分开。  </p>
<p>支持向量机的种类：      </p>
<blockquote>
<p>a) 线性可分SVM：当数据线性可分的时候，通过硬间隔最大化可以学习得到一个线性分类器，即硬间隔SVM，如上图的H3</p>
<p>b) 线性SVM:当训练数据不能线性可分但是可以近似线性可分时，通过软间隔(soft margin)最大化也可以学习到一个线性分类器，即软间隔SVM</p>
<p>c) 非线性SVM：</p>
<p>当训练数据线性不可分时，通过使用核技巧(kernel trick)和软间隔最大化，可以学习到一个非线性SVM。</p>
</blockquote>
<p>超平面与间隔：</p>
<p><img src="https://img-blog.csdnimg.cn/20200916085324603.png" alt="在这里插入图片描述"></p>
<blockquote>
<p> 我们从图上可以看到，这条中间的实线代表的超平面离直线两边的数据的间隔最大，对训练集的数据的噪声有最大的包容力。</p>
</blockquote>
<p>函数原型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, coef0=<span class="number">0.0</span>,decision_function_shape=<span class="literal">None</span>, degree=<span class="number">3</span>, gamma=‘auto’, kernel=‘rbf’,max_iter=-<span class="number">1</span>, probability=<span class="literal">False</span>, random_state=<span class="literal">None</span>, shrinking=<span class="literal">True</span>,</span><br><span class="line">tol=<span class="number">0.001</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入相关的包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl  <span class="comment"># 绘图功能</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建 40 个点</span></span><br><span class="line">np.random.seed(<span class="number">0</span>) <span class="comment"># 让每次运行程序生成的随机样本点不变</span></span><br><span class="line"><span class="comment"># 生成训练实例并保证是线性可分的</span></span><br><span class="line"><span class="comment"># np._r表示将矩阵在行方向上进行相连</span></span><br><span class="line"><span class="comment"># random.randn(a,b)表示生成 a 行 b 列的矩阵，且随机数服从标准正态分布</span></span><br><span class="line"><span class="comment"># array(20,2) - [2,2] 相当于给每一行的两个数都减去 2</span></span><br><span class="line">X = np.r_[np.random.randn(<span class="number">20</span>, <span class="number">2</span>) - [<span class="number">2</span>, <span class="number">2</span>], np.random.randn(<span class="number">20</span>, <span class="number">2</span>) + [<span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line"><span class="comment"># 两个类别 每类有 20 个点，Y 为 40 行 1 列的列向量</span></span><br><span class="line">Y = [<span class="number">0</span>] * <span class="number">20</span> + [<span class="number">1</span>] * <span class="number">20</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 建立 svm 模型</span></span><br><span class="line">clf = svm.SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">clf.fit(X, Y)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 获得划分超平面</span></span><br><span class="line"><span class="comment"># 划分超平面原方程：w0x0 + w1x1 + b = 0</span></span><br><span class="line"><span class="comment"># 将其转化为点斜式方程，并把 x0 看作 x，x1 看作 y，b 看作 w2</span></span><br><span class="line"><span class="comment"># 点斜式：y = -(w0/w1)x - (w2/w1)</span></span><br><span class="line">w = clf.coef_[<span class="number">0</span>]  <span class="comment"># w 是一个二维数据，coef 就是 w = [w0,w1]</span></span><br><span class="line">a = -w[<span class="number">0</span>] / w[<span class="number">1</span>]  <span class="comment"># 斜率</span></span><br><span class="line">xx = np.linspace(-<span class="number">5</span>, <span class="number">5</span>)  <span class="comment"># 从 -5 到 5 产生一些连续的值（随机的）</span></span><br><span class="line"><span class="comment"># .intercept[0] 获得 bias，即 b 的值，b / w[1] 是截距</span></span><br><span class="line">yy = a * xx - (clf.intercept_[<span class="number">0</span>]) / w[<span class="number">1</span>]  <span class="comment"># 带入 x 的值，获得直线方程</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 画出和划分超平面平行且经过支持向量的两条线（斜率相同，截距不同）</span></span><br><span class="line">b = clf.support_vectors_[<span class="number">0</span>] <span class="comment"># 取出第一个支持向量点</span></span><br><span class="line">yy_down = a * xx + (b[<span class="number">1</span>] - a * b[<span class="number">0</span>]) </span><br><span class="line">b = clf.support_vectors_[-<span class="number">1</span>] <span class="comment"># 取出最后一个支持向量点</span></span><br><span class="line">yy_up = a * xx + (b[<span class="number">1</span>] - a * b[<span class="number">0</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 查看相关的参数值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w: &quot;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a: &quot;</span>, a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;support_vectors_: &quot;</span>, clf.support_vectors_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;clf.coef_: &quot;</span>, clf.coef_)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 在 scikit-learin 中，coef_ 保存了线性模型中划分超平面的参数向量。形式为(n_classes, n_features)。若 n_classes &gt; 1，则为多分类问题，(1，n_features) 为二分类问题。</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 绘制划分超平面，边际平面和样本点</span></span><br><span class="line">pl.plot(xx, yy, <span class="string">&#x27;k-&#x27;</span>)</span><br><span class="line">pl.plot(xx, yy_down, <span class="string">&#x27;k--&#x27;</span>)</span><br><span class="line">pl.plot(xx, yy_up, <span class="string">&#x27;k--&#x27;</span>)</span><br><span class="line"><span class="comment"># 圈出支持向量</span></span><br><span class="line">pl.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>],</span><br><span class="line">           s=<span class="number">80</span>, facecolors=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">pl.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, cmap=pl.cm.Paired)</span><br><span class="line"> </span><br><span class="line">pl.axis(<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure>

<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><h3 id="随机森林简介"><a href="#随机森林简介" class="headerlink" title="随机森林简介"></a>随机森林简介</h3><p>随机森林应该是很多小伙伴们在学机器学习算法时最先接触到的集成算法，我们先简单介绍一下集成学习的大家族吧</p>
<ul>
<li>Bagging：个体评估器之间不存在强依赖关系，一系列个体学习器可以并行生成。代表算法：随机森林（Random Forest）</li>
<li>Boosting：个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成。代表算法：AdaBoost、GBDT、XGBoost、LightGBM</li>
</ul>
<p>在Bagging集成中，需要并行建立多个弱评估器（非线性算法），然后综合多个弱评估器的结果进行输出：</p>
<ul>
<li>Bootstrap: 从原始样本集中采用有放回抽样的方式抽取n个训练样本，共进行k轮抽取，得到k个相互独立的训练集。</li>
<li>对每个训练集进行训练，得到k个模型。</li>
<li>对分类问题：预测结果为k个分类器投票方式得到的分类结果，少数服从多数。</li>
<li>对回归问题：将学习器的均值作为预测结果。</li>
</ul>
<p>随机森林是bagging家族的代表算法，它的算法思想体现在它的名字上：“随机”和“森林”。首先“森林”是指随机森林的所有基学习器都是决策树，“随机”是指随机从原样本集中抽取样本和特征来训练，并不会使用所有的样本和特征，每棵树独立地有放回抽样，这就保证了每棵树所使用的数据集是不同的，进而所生成的树也是有差异的，最后集成所有树的决策结果，得到最终结果。具体算法流程如下：</p>
<p>从样本中随机抽取不同的子集，用于建立不同的决策树，在<strong>按照Bagging的规则对决策结果进行集成：</strong></p>
<ol>
<li>从M个的原始样本集中采用有放回抽样的方式抽取m个训练样本，共进行k轮抽取，得到k个相互独立的训练集，即有k个基学习器，注意：每颗树的特征并不是在建树前就抽好的，而是在每棵树分裂的节点进行抽样</li>
<li>用决策树算法对每个训练集进行训练，得到k个树</li>
<li>对分类问题：预测结果为k个分类器投票方式得到的分类结果，少数服从多数</li>
<li>对回归问题：将学习器的均值作为预测结果</li>
</ol>
<h3 id="导入各种包"><a href="#导入各种包" class="headerlink" title="导入各种包"></a>导入各种包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split  </span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier   </span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc </span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">24</span></span><br></pre></td></tr></table></figure>

<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>为了方便大家代码复现，本次使用的是python自带的泰坦尼克号数据集，共981个样本，特征涉及性别、年龄、船票价格、是否有同伴等等，标签列有两个，分别是‘survived’和‘alive’，都表示该乘客是否生还，所以我们取一列就可以了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = sns.load_dataset(<span class="string">&#x27;titanic&#x27;</span>)  <span class="comment"># 导入泰坦尼克号生还数据</span></span><br><span class="line">data</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/a8d74c7088ff4eb59deec83ccd2c7cc2.png" alt="img"></p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.replace(to_replace=<span class="string">r&#x27;^\s*$&#x27;</span>, value=np.nan, regex=<span class="literal">True</span>, inplace=<span class="literal">True</span>)   <span class="comment"># 把各类缺失类型统一改为NaN的形式</span></span><br><span class="line">data.isnull().mean()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/02146917e65648aaa1a75272c6685929.png" alt="img"></p>
<p>共4列数据存在缺失值，‘deck’缺失率超过70%，予以删除，剩余特征的缺失值使用其均值或是众数进行填补</p>
<p>细心地童鞋可能发现了有好几列重复的特征，‘embarked’和‘embark_town’都表示出发港口，‘sex’、‘who’、‘adult_male’都表示性别，‘pclass’和‘class’都是船票类型，‘sibsp’和‘alone’都表示是否有同伴，对于这几个特征，所以我们保留其中一个就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;deck&#x27;</span>]   <span class="comment"># 删除‘deck’列</span></span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;who&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;adult_male&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;class&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;alone&#x27;</span>]</span><br><span class="line"> </span><br><span class="line">data[<span class="string">&#x27;age&#x27;</span>].fillna(np.mean(data.age), inplace=<span class="literal">True</span>)   <span class="comment"># 年龄特征使用均值对缺失值进行填补</span></span><br><span class="line">data[<span class="string">&#x27;embarked&#x27;</span>].fillna(data[<span class="string">&#x27;embarked&#x27;</span>].mode(dropna=<span class="literal">False</span>)[<span class="number">0</span>], inplace=<span class="literal">True</span>)   <span class="comment"># 文本型特征视同众数进行缺失值填补</span></span><br><span class="line"> </span><br><span class="line">x = data.drop([<span class="string">&#x27;alive&#x27;</span>, <span class="string">&#x27;survived&#x27;</span>, <span class="string">&#x27;embark_town&#x27;</span>], axis=<span class="number">1</span>)   <span class="comment"># 取出用于建模的特征列X</span></span><br><span class="line">label = data[<span class="string">&#x27;survived&#x27;</span>]   <span class="comment"># 取出标签列Y</span></span><br></pre></td></tr></table></figure>

<p> sklean中的随机森林算法是无法进行字符串的处理的，所以要先进行数据编码，这里我们就使用最简单的特征编码，转化完毕后特征全部变为数值型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">oe = OrdinalEncoder()   <span class="comment"># 定义特征转化函数</span></span><br><span class="line"> <span class="comment"># 把需要转化的特征都写进去</span></span><br><span class="line">x[[<span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;embarked&#x27;</span>]] = oe.fit_transform(x[[<span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;embarked&#x27;</span>]])  </span><br><span class="line">x.head()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/d57f05fc499f42dca2e0ced7feab38a2.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#划分训练集、测试集</span></span><br><span class="line">xtrain, xtest, ytrain, ytest = train_test_split(x, label, test_size=<span class="number">0.3</span>)</span><br><span class="line">xtrain.head()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/7e5dff4b5f9c4a0da56e432df49adeaa.png" alt="img"></p>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">随机森林所有超参数</span></span><br><span class="line"><span class="string">sklearn.ensemble.RandomForestClassifier (n_estimators=100, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, </span></span><br><span class="line"><span class="string">                                         min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, </span></span><br><span class="line"><span class="string">                                         min_impurity_split=None, class_weight=None, random_state=None, bootstrap=True, oob_score=False, </span></span><br><span class="line"><span class="string">                                         n_jobs=None, verbose=0, warm_start=False)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 单颗决策树</span></span><br><span class="line">clf = DecisionTreeClassifier(class_weight=<span class="string">&#x27;balanced&#x27;</span>,random_state=<span class="number">37</span>)   </span><br><span class="line">clf = clf.fit(xtrain, ytrain)   <span class="comment"># 拟合训练集</span></span><br><span class="line">score_c = clf.score(xtest, ytest)   <span class="comment"># 输出测试集准确率</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机森林</span></span><br><span class="line">rfc = RandomForestClassifier(class_weight=<span class="string">&#x27;balanced&#x27;</span>,random_state=<span class="number">37</span>)   </span><br><span class="line">rfc = rfc.fit(xtrain, ytrain)</span><br><span class="line">score_r = rfc.score(xtest, ytest)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/4183b32c94d1428ab35068a9eb569544.png" alt="img"></p>
<p>这里的score封装的是准确率，即（模型预测正确的样本量）/（模型的总样本量），因此容易受到样本不均匀的影响，尤其是像癌症预测、信贷逾期预测之类的模型，响应样本的占比就很少，100个人中可能就1个人会有逾期的行为，对于这种情况，我们应该使用AUC这类评价指标，可以平衡样本量偏颇带来的影响，那下面我们来看一看单科决策树和随机森林的AUC情况吧。</p>
<h3 id="模型效果"><a href="#模型效果" class="headerlink" title="模型效果"></a>模型效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 决策树 预测测试集</span></span><br><span class="line">y_test_proba_clf = clf.predict_proba(xtest)</span><br><span class="line">false_positive_rate_clf, recall_clf, thresholds_clf = roc_curve(ytest, y_test_proba_clf[:, <span class="number">1</span>])  </span><br><span class="line"><span class="comment"># 决策树 AUC指标</span></span><br><span class="line">roc_auc_clf = auc(false_positive_rate_clf, recall_clf)     </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机森林 预测测试集</span></span><br><span class="line">y_test_proba_rfc = rfc.predict_proba(xtest)</span><br><span class="line">false_positive_rate_rfc, recall_rfc, thresholds_rfc = roc_curve(ytest, y_test_proba_rfc[:, <span class="number">1</span>])  </span><br><span class="line"><span class="comment"># 随机森林 AUC指标</span></span><br><span class="line">roc_auc_rfc = auc(false_positive_rate_rfc, recall_rfc)     </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 画图 画出俩模型的ROC曲线</span></span><br><span class="line">plt.plot(false_positive_rate_clf, recall_clf, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;AUC_clf=%0.3f&#x27;</span> % roc_auc_clf) </span><br><span class="line">plt.plot(false_positive_rate_rfc, recall_rfc, color=<span class="string">&#x27;orange&#x27;</span>, label=<span class="string">&#x27;AUC_rfc=%0.3f&#x27;</span> % roc_auc_rfc)  </span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>, fontsize=<span class="number">15</span>, frameon=<span class="literal">False</span>)  </span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;r--&#x27;</span>)  </span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])  </span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])  </span><br><span class="line">plt.ylabel(<span class="string">&#x27;Recall&#x27;</span>)  </span><br><span class="line">plt.xlabel(<span class="string">&#x27;Fall-out&#x27;</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/c6eafdedff444aaea2a776652b9f6a47.png" alt="img"></p>
<p>来瞧一瞧看一看了昂，黄色实线代表随机森林的ROC曲线，蓝色实线代表单棵决策树的ROC曲线，在俩模型均未调参的情况下，明显随机森林的模型效果要比单科决策树的好对吧，所以集成算法还是牛的~</p>
<h3 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h3><p>接下来我们尝试通过调整随机森林的超参数，来提高模型的性能，评价指标设为AUC</p>
<p>由于随机森林的基学习器均为决策树，因此我们调参的方向也和决策树相同，主要通过控制剪枝的相关参数来防止模型过拟合，这里我们选择两个参数进行调整，分别是：基学习器数量n_estimators和树深max_depth，大家可以使用网格搜索等自动调参工具，这里为了演示调参过程，我们来手动调整参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义空列表，用来存放每一个基学习器数量所对应的AUC值</span></span><br><span class="line">superpa = []</span><br><span class="line"><span class="comment"># 循环200次</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    rfc = ensemble.RandomForestClassifier(n_estimators=i+<span class="number">1</span>, class_weight=<span class="string">&#x27;balanced&#x27;</span>,random_state=<span class="number">37</span>, n_jobs=<span class="number">10</span>)</span><br><span class="line">    rfc = rfc.fit(xtrain, ytrain)   <span class="comment"># 拟合模型</span></span><br><span class="line">    </span><br><span class="line">    y_test_proba_rfc = rfc.predict_proba(xtest)   <span class="comment"># 预测测试集</span></span><br><span class="line">    false_positive_rate_rfc, recall_rfc, thresholds_rfc = roc_curve(ytest, y_test_proba_rfc[:, <span class="number">1</span>])  </span><br><span class="line">    roc_auc_rfc = auc(false_positive_rate_rfc, recall_rfc)   <span class="comment"># 计算模型AUC</span></span><br><span class="line">    </span><br><span class="line">    superpa.append(roc_auc_rfc)   <span class="comment"># 记录每一轮的AUC值</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">max</span>(superpa),superpa.index(<span class="built_in">max</span>(superpa)))   <span class="comment"># 输出最大的AUC值和其对应的轮数</span></span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">201</span>),superpa)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/b88ac185b89744b396343955ad5e0198.png" alt="img"></p>
<p>最大AUC为0.8871，对应的轮数是107，即基学习器数量为108时AUC取得最大值0.8871，接下来固定基学习器个数，调整最大树深</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">superpa = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    rfc = ensemble.RandomForestClassifier(max_depth=i+<span class="number">1</span>, n_estimators=<span class="number">185</span>,</span><br><span class="line">                                          class_weight=<span class="string">&#x27;balanced&#x27;</span>, random_state=<span class="number">37</span>, n_jobs=<span class="number">10</span>)</span><br><span class="line">    rfc = rfc.fit(xtrain, ytrain)</span><br><span class="line">    </span><br><span class="line">    y_test_proba_rfc = rfc.predict_proba(xtest)</span><br><span class="line">    false_positive_rate_rfc, recall_rfc, thresholds_rfc = roc_curve(ytest, y_test_proba_rfc[:, <span class="number">1</span>])  </span><br><span class="line">    roc_auc_rfc = auc(false_positive_rate_rfc, recall_rfc) </span><br><span class="line">    </span><br><span class="line">    superpa.append(roc_auc_rfc)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">max</span>(superpa),superpa.index(<span class="built_in">max</span>(superpa)))</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">21</span>),superpa)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/236d1ee310c24c9d9697bd5f4a753ee8.png" alt="img"></p>
<p>i=6即树深为7的时候，AUC达到最值0.8983</p>
<p>到此我们调参就结束了，我们通过手动调参，先找到最佳基学习器数量，在此基础上在寻找最佳树深， 但实际上不同的参数组合带来的效果是不同的，啥意思呢，举例说：</p>
<p>我们这边先固定学习器的数量为108，再去找树深，然后我们认为（学习器数量108+树深7）为最佳组合，但是当我们的树深为7时，基学习器的最佳选择可能就不是108了，因为这个108是在不限制树深的情况下跑出来的结果</p>
<p>所以自动调参机器网格搜索就是去枚举尽可能多的参数组合，从中找出最佳的，来弥补手动调参的弊端，但是随着超参数的增加，参数组合可以说是无穷无尽，所以网格搜索也只是尽量找到最佳组合，调参这个工作还是看经验，调的多了慢慢就有感觉了，毕竟调参是门玄学O(∩_∩)O哈哈~</p>
<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><p>今天要说的Adaboost算是boosting中比较元老级别的算法了，我们先来说说boosting算法的特点吧</p>
<ul>
<li>它和bagging算法不同，boosting是通过降低整体的偏差来降低泛化误差，因此被称为提升法</li>
<li>相对于bagging，boosting算法在原理和操作上难度都更大</li>
<li>由于boosting算法专注于降低与真实值之间的偏差，因此boosting在模型效果上表现突出</li>
</ul>
<p>Adaboost(Adaptive Boosting, 自适应增强)是较为早期的boosting算法，所以他的构筑过程相对于其他boosting算法来说都是较为简单直观的，如果你不能够透彻地理解gbdt或是xgboost的原理，可以先从Adaboost入手，懂弄了之后再去看更复杂的集成算法，可能就不会很困难啦！</p>
<p>Adaboost的算法原理如下：</p>
<ol>
<li>基于全部样本建立一颗决策树</li>
<li>根据第一颗决策树的预测结果与真实值之间的偏差，增加被预测错误的样本在原数据集中的权重，并让加权后的全部样</li>
<li>作为下一颗决策树的训练样本</li>
<li>用加权后的数据建立第二颗树，查看第二棵树被预测错误的样本，根据这个结果修正样本权重用于第三棵树</li>
<li>一直循环上述过程，直到预测误差小于设定的阈值内，终止迭代，停止生成树</li>
</ol>
<p>在理解Adaboost算法的时候，有两点需要注意的地方哦：①和随机森林不同，Adaboost不进行样本和特征的抽样，会用全部样本和特征来建立每一个基学习器；②整个算法过程中涉及两个权重的迭代更新，一个是样本权重，根据每轮基学习器的预测结果，增加预测错误样本对应的权重，实质上就是我们在建立下一个基学习器时，会更重视上一轮被预测错误的样本，尽量避免它在这一轮又被预测错误，以此来不断修正错误，进而我们的集成所有的基学习器后，正确率就会高很多，还有一个权重是每一个基学习器的权重，这个权重由每一个基学习器的分类误差率决定，分类误差率越低，基学习器的权重越高，说白了就是能力越大责任越大，预测的准的基学习器就给它话语权，它说了算</p>
<p>具体算法公式啥的大家就自行学习理解叭，我们今天主要是说如何使用sklearn包来实现Adaboost以及简单的调参演示，话不多说上代码~</p>
<h3 id="导入各种包-1"><a href="#导入各种包-1" class="headerlink" title="导入各种包"></a>导入各种包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc </span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier  </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> graphviz </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">24</span></span><br></pre></td></tr></table></figure>

<h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><p>为了方便大家代码复现，本次使用的是python自带的泰坦尼克号数据集，共981个样本，特征涉及性别、年龄、船票价格、是否有同伴等等，标签列有两个，分别是‘survived’和‘alive’，都表示该乘客是否生还，所以我们取一列就可以了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = sns.load_dataset(&#x27;titanic&#x27;)  # 导入泰坦尼克号生还数据</span><br><span class="line">data</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/35b01a01ac574f0aaeec1f3e57e81d28.png" alt="img"></p>
<h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p> 首先进行数据的预处理，首先可以看到‘deck’列存在缺失值，我们可以看看总体数据缺失的情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.replace(to_replace=r&#x27;^\s*$&#x27;, value=np.nan, regex=True, inplace=True)   # 把各类缺失类型统一改为NaN的形式</span><br><span class="line">data.isnull().mean()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/84327b8961134a2e8d175981bdcbe520.png" alt="img"></p>
<p>共4列数据存在缺失值，‘deck’缺失率超过70%，予以删除，剩余特征的缺失值使用其均值或是众数进行填补</p>
<p>细心地童鞋可能发现了有好几列重复的特征，‘embarked’和‘embark_town’都表示出发港口，‘sex’、‘who’、‘adult_male’都表示性别，‘pclass’和‘class’都是船票类型，‘sibsp’和‘alone’都表示是否有同伴，对于这几个特征，所以我们保留其中一个就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;deck&#x27;</span>]   <span class="comment"># 删除‘deck’列</span></span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;who&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;adult_male&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;class&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">&#x27;alone&#x27;</span>]</span><br><span class="line"> </span><br><span class="line">data[<span class="string">&#x27;age&#x27;</span>].fillna(np.mean(data.age), inplace=<span class="literal">True</span>)   <span class="comment"># 年龄特征使用均值对缺失值进行填补</span></span><br><span class="line">data[<span class="string">&#x27;embarked&#x27;</span>].fillna(data[<span class="string">&#x27;embarked&#x27;</span>].mode(dropna=<span class="literal">False</span>)[<span class="number">0</span>], inplace=<span class="literal">True</span>)   <span class="comment"># 文本型特征视同众数进行缺失值填补</span></span><br><span class="line"> </span><br><span class="line">x = data.drop([<span class="string">&#x27;alive&#x27;</span>, <span class="string">&#x27;survived&#x27;</span>, <span class="string">&#x27;embark_town&#x27;</span>], axis=<span class="number">1</span>)   <span class="comment"># 取出用于建模的特征列X</span></span><br><span class="line">label = data[<span class="string">&#x27;survived&#x27;</span>]   <span class="comment"># 取出标签列Y</span></span><br></pre></td></tr></table></figure>

<p>sklean中的Adaboost算法是无法进行字符串的处理的，所以要先进行数据编码，这里我们就使用最简单的特征编码，转化完毕后特征全部变为数值型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">oe = OrdinalEncoder()   <span class="comment"># 定义特征转化函数</span></span><br><span class="line"> <span class="comment"># 把需要转化的特征都写进去</span></span><br><span class="line">x[[<span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;embarked&#x27;</span>]] = oe.fit_transform(x[[<span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;embarked&#x27;</span>]])  </span><br><span class="line">x.head()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/d7d1e17b17b043cdb53eef2111b820c0.png" alt="img"></p>
<p>划分训练集、测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xtrain, xtest, ytrain, ytest = train_test_split(x, label, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>

<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">slearn封装的Adaboost及其参数</span></span><br><span class="line"><span class="string">sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=&#x27;SAMME.R&#x27;, random_state=None)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 这里同时训练了Adaboost分类模型和随机森林分类模型，用于对比模型效果</span></span><br><span class="line">rfc = RandomForestClassifier(class_weight=<span class="string">&#x27;balanced&#x27;</span>, random_state=<span class="number">37</span>)  <span class="comment"># 随机森林</span></span><br><span class="line">rfc = rfc.fit(xtrain, ytrain)</span><br><span class="line">score_r = rfc.score(xtest, ytest)</span><br><span class="line"> </span><br><span class="line">abc = AdaBoostClassifier(random_state=<span class="number">37</span>)  <span class="comment"># adaboost</span></span><br><span class="line">abc = abc.fit(xtrain, ytrain)  <span class="comment"># 拟合训练集</span></span><br><span class="line">score_a = abc.score(xtest, ytest)  <span class="comment"># 输出测试集准确率</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;随机森林:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(score_r), <span class="string">&quot; adaboost:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(score_a))</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/c4914d82a46d4dde8933402df659850b.png" alt="img"></p>
<p>从准确率这个指标来看，随机森林略胜一筹，但毕竟是分类问题，我们再使用AUC来评估模型，看一看是否随机森林效果更好</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">y_test_proba_rfc = rfc.predict_proba(xtest)</span><br><span class="line">false_positive_rate_rfc, recall_rfc, thresholds_rfc = roc_curve(ytest, y_test_proba_rfc[:, <span class="number">1</span>])</span><br><span class="line">roc_auc_rfc = auc(false_positive_rate_rfc, recall_rfc)  <span class="comment"># 随机森林AUC指标</span></span><br><span class="line"> </span><br><span class="line">y_test_proba_abc = abc.predict_proba(xtest)</span><br><span class="line">false_positive_rate_abc, recall_abc, thresholds_abc = roc_curve(ytest, y_test_proba_abc[:, <span class="number">1</span>])</span><br><span class="line">roc_auc_abc = auc(false_positive_rate_abc, recall_abc)  <span class="comment"># adaboost AUC指标</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 画出俩模型对应的ROC曲线</span></span><br><span class="line">plt.plot(false_positive_rate_rfc, recall_rfc, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;AUC_rfc=%0.3f&#x27;</span> % roc_auc_rfc)</span><br><span class="line">plt.plot(false_positive_rate_abc, recall_abc, color=<span class="string">&#x27;orange&#x27;</span>, label=<span class="string">&#x27;AUC_abc=%0.3f&#x27;</span> % roc_auc_abc)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>, fontsize=<span class="number">15</span>, frameon=<span class="literal">False</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;r--&#x27;</span>)</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Recall&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Fall-out&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/75e26c1c34c14d0288d9076613f5976a.png" alt="img"></p>
<p>蓝色实线为随机森林的ROC曲线，黄色实线为Adaboost的ROC曲线，从AUC指标评估模型的话，随机森林效果是要更好的，而且AUC指标对于分类模型的评估还是很公平有效的</p>
<h3 id="调参-1"><a href="#调参-1" class="headerlink" title="调参"></a>调参</h3><p>这里设定Adaboost的基学习器为决策树，最大深度为2，当然你也可以设为别的分类算法，通过参数base_estimator进行设置，集成算法最重要的俩参数就是基学习器数量n_estimators和学习率learning_rate，这里我们就通过调整这两个参数来提升模型效果</p>
<p>网格搜索类似枚举法，把要调整的参数和参数范围设置完毕之后，它可以进行参数组合，找到模型效果最佳的模型组合，当需要调整的参数较多或参数范围很广时，网格搜索就会非常慢，有利有弊叭，所以一般的调参可以先手动调一调，找到参数大致的最优范围，再通过网格搜索去准确定位最优参数值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 通过网格搜索法选择合理的Adaboost算法参数</span><br><span class="line">n_estimators = [10, 100, 200, 300, 400, 500, 600]</span><br><span class="line">learning_rate = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5]</span><br><span class="line">params2 = &#123;&#x27;n_estimators&#x27;:n_estimators,&#x27;learning_rate&#x27;:learning_rate&#125;</span><br><span class="line">adaboost = GridSearchCV(estimator = ensemble.AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2), </span><br><span class="line">                            algorithm=&quot;SAMME&quot;, random_state=37), </span><br><span class="line">                        param_grid= params2, scoring = &#x27;roc_auc&#x27;, cv = 5, n_jobs = 10, verbose = 1)</span><br><span class="line">adaboost.fit(xtrain, ytrain)</span><br><span class="line"> </span><br><span class="line">print(&#x27;best_params_:&#x27;, adaboost.best_params_) # 返回参数的最佳组合和对应AUC值</span><br><span class="line">print(&#x27;best_score_:&#x27;, adaboost.best_score_)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/e84db8322722426ca41de7d6b7b0280e.png" alt="img"></p>
<p>这里给出的最佳参数为100个基学习器搭配0.2的学习率，AUC达到0.86，那我们就认为这是最佳的参数组合，带入模型看看效果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">abc = ensemble.AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),</span><br><span class="line">                                  n_estimators=100, learning_rate=0.2, algorithm=&quot;SAMME&quot;, random_state=37)</span><br><span class="line">abc = abc.fit(xtrain, ytrain)</span><br><span class="line"> </span><br><span class="line">y_test_proba_abc = abc.predict_proba(xtest)</span><br><span class="line">false_positive_rate_abc, recall_abc, thresholds_abc = roc_curve(ytest, y_test_proba_abc[:, 1])  </span><br><span class="line">auc(false_positive_rate_abc, recall_abc)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/9dcc350728eb48f2a7be0175fe9bce45.png" alt="img"></p>
<p>带入模型后可以看到AUC值为0.87，和刚才的0.86并不相同，因为我们在调参时，设置了CV=5，就是使用了5折交叉验证，把数据分为五份，每一份都去训练评估得到一个AUC值，而这个0.86就是这5个AUC值得均值，后面把调参出来的组合带入模型后，使用的是原始的训练集合测试集，一般来说得到的AUC值都会产生波动，高点或者低点都是很正常的</p>
<h2 id="聚类算法（K-means）"><a href="#聚类算法（K-means）" class="headerlink" title="聚类算法（K-means）"></a>聚类算法（K-means）</h2><h3 id="K-means算法的流程："><a href="#K-means算法的流程：" class="headerlink" title="K-means算法的流程："></a>K-means算法的流程：</h3><p>1.随机选取K个中心点</p>
<p>2.遍历数据集里面的每个点，看距离哪个中心点最近就分为哪一类，遍历完一共K类</p>
<p>3.把属于一类的点取平均值，得到的平均值作为新的中心点</p>
<p>4.然后不断重复步骤2，3，直到达到结束条件为止。（当中心点不再变动或变动很小，当达到最大迭代次数）</p>
<h3 id="K-means算法的优缺点及算法复杂度"><a href="#K-means算法的优缺点及算法复杂度" class="headerlink" title="K-means算法的优缺点及算法复杂度"></a>K-means算法的优缺点及算法复杂度</h3><p><strong>优点：</strong></p>
<p>原理简单，算法复杂度较低</p>
<p><strong>缺点：</strong></p>
<p>k值未知，需要人为设定</p>
<p>对于初始化中心点特别敏感，不同的初始化，结果可能不一样</p>
<p>容易受到噪声的影响，可能收敛于局部最小值，同时数据量大时收敛速度较慢</p>
<p>不太适合离散的数据，样本类别不均衡的数据的聚类</p>
<p>k-means 有一个重要特征，它要求这些簇的模型必须是圆形：k-means 算法没有内置的方法<br>来实现椭圆形的簇</p>
<p><strong>算法复杂度：</strong></p>
<p>O(tkn*d)  t是迭代次数，k是类数，n是数据点个数，d是数据维度</p>
<h3 id="K-means算法的调优与改进"><a href="#K-means算法的调优与改进" class="headerlink" title="K-means算法的调优与改进"></a>K-means算法的调优与改进</h3><p>k值的选取问题，K-means++，或者先使用谱聚类或层次聚类对样本进行聚类，得到K</p>
<p>，或使用手肘法，遍历可能的K值，画出该点下Loss的大小，选择曲线的拐点处的K值</p>
<p>对于数据量大的情况，可以选择mini-batch的方法，不过准确度会下降</p>
<p>初始点敏感的问题，可以选择多种初始点情况，选择误差最小的一种</p>
<p>噪声影响问题，K-medoids，将步骤3改为，求一个类里面，每个点到类内其他点距离和最小的，选择它作为我们下一步的中心点，这样就有效缓解了噪声问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x,y = make_blobs(n_samples=<span class="number">1000</span>,n_features=<span class="number">4</span>,centers=[[-<span class="number">1</span>,-<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]],cluster_std=[<span class="number">0.4</span>,<span class="number">0.2</span>,<span class="number">0.2</span>,<span class="number">0.4</span>],random_state=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">k_means = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">k_means.fit(x)</span><br><span class="line"></span><br><span class="line">y_predict = k_means.predict(x)</span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y_predict)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(k_means.predict((x[:<span class="number">30</span>,:])))</span><br><span class="line"><span class="built_in">print</span>(metrics.calinski_harabasz_score(x,y_predict))</span><br><span class="line"><span class="built_in">print</span>(k_means.cluster_centers_)</span><br><span class="line"><span class="built_in">print</span>(k_means.inertia_)</span><br><span class="line"><span class="built_in">print</span>(metrics.silhouette_score(x,y_predict))</span><br><span class="line"><span class="comment">#结果</span></span><br><span class="line">[<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"><span class="number">2672.175134496046</span></span><br><span class="line">[[-<span class="number">0.98579917</span> -<span class="number">1.04421422</span>]</span><br><span class="line"> [ <span class="number">1.4925044</span>   <span class="number">1.49887711</span>]</span><br><span class="line"> [-<span class="number">0.03211515</span> -<span class="number">0.01417351</span>]]</span><br><span class="line"><span class="number">415.81167375689665</span></span><br><span class="line"><span class="number">0.5758167373145309</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221207161835.png?raw=true" alt="blog_images20221207161835.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">山不让尘，川不辞盈</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">山不让尘，川不辞盈</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/21/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"><img class="prev-cover" src="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">数据预处理</div></div></a></div><div class="next-post pull-right"><a href="/2022/10/19/QUIC%E5%8D%8F%E8%AE%AE/"><img class="next-cover" src="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">QUIC协议</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81NzQzNS8zMzg5OQ=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F14071825039%2F641&amp;refer=http%3A%2F%2Finews.gtimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668341762&amp;t=8f1764bb17a8a475a7658fd817bf1965" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">山不让尘，川不辞盈</div><div class="author-info__description">归途也还可爱</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/446773160"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/446773160" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/446773160@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_43762551?type=blog" target="_blank" title="CSDN"><i class="fa-solid fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到山不让尘,川不辞盈的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E6%8F%90"><span class="toc-number">1.</span> <span class="toc-text">前提</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">什么是机器学习？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">3.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.1.</span> <span class="toc-text">监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.2.</span> <span class="toc-text">无监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.3.</span> <span class="toc-text">强化学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.</span> <span class="toc-text">半监督学习与主动学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B8%89%E8%A6%81%E7%B4%A0"><span class="toc-number">4.</span> <span class="toc-text">机器学习方法三要素</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5"><span class="toc-number">4.2.</span> <span class="toc-text">策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">5.</span> <span class="toc-text">模型评估与模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="toc-number">5.1.</span> <span class="toc-text">训练误差与测试误差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">5.2.</span> <span class="toc-text">过拟合与模型选择</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">6.</span> <span class="toc-text">正则化与交叉验证</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">6.2.</span> <span class="toc-text">交叉验证</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-number">7.</span> <span class="toc-text">泛化能力</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.1.</span> <span class="toc-text">生成模型与判别模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8"><span class="toc-number">8.</span> <span class="toc-text">监督学习应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">8.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98"><span class="toc-number">8.2.</span> <span class="toc-text">标注问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">8.3.</span> <span class="toc-text">回归问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sklearn"><span class="toc-number">9.</span> <span class="toc-text">sklearn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn%E6%A6%82%E8%BF%B0"><span class="toc-number">9.1.</span> <span class="toc-text">sklearn概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn%E6%A8%A1%E5%9D%97"><span class="toc-number">9.2.</span> <span class="toc-text">sklearn模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86-Preprocessing"><span class="toc-number">9.3.</span> <span class="toc-text">预处理(Preprocessing)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96"><span class="toc-number">9.3.1.</span> <span class="toc-text">数据无量纲化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">9.3.2.</span> <span class="toc-text">缺失值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%9E%8B%E7%89%B9%E5%BE%81%EF%BC%9A%E7%BC%96%E7%A0%81%E4%B8%8E%E5%93%91%E5%8F%98%E9%87%8F"><span class="toc-number">9.3.3.</span> <span class="toc-text">处理分类型特征：编码与哑变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%89%B9%E5%BE%81%EF%BC%9A%E4%BA%8C%E5%80%BC%E5%8C%96%E4%B8%8E%E5%88%86%E6%AE%B5"><span class="toc-number">9.3.4.</span> <span class="toc-text">处理连续型特征：二值化与分段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Filter%E8%BF%87%E6%BB%A4%E6%B3%95"><span class="toc-number">9.3.5.</span> <span class="toc-text">Filter过滤法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7%E8%BF%87%E6%BB%A4"><span class="toc-number">9.3.6.</span> <span class="toc-text">相关性过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedded%E5%B5%8C%E5%85%A5%E6%B3%95"><span class="toc-number">9.3.7.</span> <span class="toc-text">Embedded嵌入法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Wrapper%E5%8C%85%E8%A3%85%E6%B3%95"><span class="toc-number">9.3.8.</span> <span class="toc-text">Wrapper包装法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95"><span class="toc-number">9.4.</span> <span class="toc-text">KNN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">9.5.</span> <span class="toc-text">决策树算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">9.6.</span> <span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95"><span class="toc-number">9.7.</span> <span class="toc-text">朴素贝叶斯算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic%E5%9B%9E%E5%BD%92"><span class="toc-number">9.8.</span> <span class="toc-text">logistic回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM%E7%AE%97%E6%B3%95"><span class="toc-number">9.9.</span> <span class="toc-text">SVM算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">9.10.</span> <span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%80%E4%BB%8B"><span class="toc-number">9.10.1.</span> <span class="toc-text">随机森林简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E5%90%84%E7%A7%8D%E5%8C%85"><span class="toc-number">9.10.2.</span> <span class="toc-text">导入各种包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">9.10.3.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">9.10.4.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.10.5.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C"><span class="toc-number">9.10.6.</span> <span class="toc-text">模型效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E5%8F%82"><span class="toc-number">9.10.7.</span> <span class="toc-text">调参</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adaboost"><span class="toc-number">9.11.</span> <span class="toc-text">Adaboost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E5%90%84%E7%A7%8D%E5%8C%85-1"><span class="toc-number">9.11.1.</span> <span class="toc-text">导入各种包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="toc-number">9.11.2.</span> <span class="toc-text">数据读取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="toc-number">9.11.3.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">9.11.4.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E5%8F%82-1"><span class="toc-number">9.11.5.</span> <span class="toc-text">调参</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%EF%BC%88K-means%EF%BC%89"><span class="toc-number">9.12.</span> <span class="toc-text">聚类算法（K-means）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means%E7%AE%97%E6%B3%95%E7%9A%84%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-number">9.12.1.</span> <span class="toc-text">K-means算法的流程：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E5%8F%8A%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">9.12.2.</span> <span class="toc-text">K-means算法的优缺点及算法复杂度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means%E7%AE%97%E6%B3%95%E7%9A%84%E8%B0%83%E4%BC%98%E4%B8%8E%E6%94%B9%E8%BF%9B"><span class="toc-number">9.12.3.</span> <span class="toc-text">K-means算法的调优与改进</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/03/05/AQS%20%E4%BB%8B%E7%BB%8D/" title="JavaAQS介绍"><img src="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JavaAQS介绍"/></a><div class="content"><a class="title" href="/2023/03/05/AQS%20%E4%BB%8B%E7%BB%8D/" title="JavaAQS介绍">JavaAQS介绍</a><time datetime="2023-03-05T07:00:00.000Z" title="发表于 2023-03-05 15:00:00">2023-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/05/%E5%81%8F%E5%90%91%E9%94%81%E3%80%81%E8%BD%BB%E9%87%8F%E7%BA%A7%E9%94%81%E3%80%81%E9%87%8D%E9%87%8F%E7%BA%A7%E9%94%81%E3%80%81%E8%87%AA%E6%97%8B%E9%94%81%E3%80%81%E8%87%AA%E9%80%82%E5%BA%94%E8%87%AA%E6%97%8B%E9%94%81/" title="Java锁机制"><img src="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Java锁机制"/></a><div class="content"><a class="title" href="/2023/03/05/%E5%81%8F%E5%90%91%E9%94%81%E3%80%81%E8%BD%BB%E9%87%8F%E7%BA%A7%E9%94%81%E3%80%81%E9%87%8D%E9%87%8F%E7%BA%A7%E9%94%81%E3%80%81%E8%87%AA%E6%97%8B%E9%94%81%E3%80%81%E8%87%AA%E9%80%82%E5%BA%94%E8%87%AA%E6%97%8B%E9%94%81/" title="Java锁机制">Java锁机制</a><time datetime="2023-03-05T07:00:00.000Z" title="发表于 2023-03-05 15:00:00">2023-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/05/%E6%90%AD%E5%BB%BASpringBoot%E5%88%86%E5%B8%83%E5%BC%8F%E9%A1%B9%E7%9B%AE/" title="SpringBoot"><img src="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SpringBoot"/></a><div class="content"><a class="title" href="/2023/03/05/%E6%90%AD%E5%BB%BASpringBoot%E5%88%86%E5%B8%83%E5%BC%8F%E9%A1%B9%E7%9B%AE/" title="SpringBoot">SpringBoot</a><time datetime="2023-03-05T07:00:00.000Z" title="发表于 2023-03-05 15:00:00">2023-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/05/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7/" title="论文总结"><img src="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文总结"/></a><div class="content"><a class="title" href="/2023/03/05/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7/" title="论文总结">论文总结</a><time datetime="2023-03-05T07:00:00.000Z" title="发表于 2023-03-05 15:00:00">2023-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/05/JWT%E8%AF%A6%E8%A7%A3/" title="Java Web Token详解"><img src="https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Java Web Token详解"/></a><div class="content"><a class="title" href="/2023/03/05/JWT%E8%AF%A6%E8%A7%A3/" title="Java Web Token详解">Java Web Token详解</a><time datetime="2023-03-05T07:00:00.000Z" title="发表于 2023-03-05 15:00:00">2023-03-05</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic2.zhimg.com/v2-4d2704ebfd9b58700fbbf3e32200f49b_r.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 山不让尘，川不辞盈</div><div class="footer_custom_text">Hi, welcome to My Blog</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><div class="aplayer no-destroy" data-id="7422861869" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-lrcType="-1"> </div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "https://446773160.github.io/";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (true) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>