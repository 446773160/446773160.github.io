<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>机器学习 | 山不让尘，川不辞盈</title><meta name="author" content="山不让尘，川不辞盈"><meta name="copyright" content="山不让尘，川不辞盈"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习算法">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="山不让尘，川不辞盈">
<meta property="og:description" content="机器学习算法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&refer=http%3A%2F%2Fup.enterdesk.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1668339555&t=953420e9c4344774da896a7388445605">
<meta property="article:published_time" content="2022-10-21T07:00:00.000Z">
<meta property="article:modified_time" content="2022-11-16T08:12:05.670Z">
<meta property="article:author" content="山不让尘，川不辞盈">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&refer=http%3A%2F%2Fup.enterdesk.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1668339555&t=953420e9c4344774da896a7388445605"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 山不让尘，川不辞盈","link":"链接: ","source":"来源: 山不让尘，川不辞盈","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-11-16 16:12:05'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="山不让尘，川不辞盈" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F14071825039%2F641&amp;refer=http%3A%2F%2Finews.gtimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668341762&amp;t=8f1764bb17a8a475a7658fd817bf1965" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">山不让尘，川不辞盈</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-21T07:00:00.000Z" title="发表于 2022-10-21 15:00:00">2022-10-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-11-16T08:12:05.670Z" title="更新于 2022-11-16 16:12:05">2022-11-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>43分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><p><strong>本文介绍机器学习算法，全文采用python代码去写且使用numpy,pandas以及matplotlib。机器学习算法采用sklearn框架去写，本文也会介绍机器学习框架的算法。如果基础不好的同学请移步去学习python基础。</strong></p>
<h1 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h1><p><strong>维基百科：</strong></p>
<p>机器学习是近20多年兴起的一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与统计推断学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。</p>
<p><strong>网络教学信息</strong></p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">斯坦福机器学习</span><br><span class="line">http://v.163.com/special/opencourse/machinelearning.html</span><br><span class="line">CMU 机器学习课程</span><br><span class="line">http://www.cs.cmu.edu/~epxing/Class/10715/  </span><br><span class="line">http://www.cs.cmu.edu/~epxing/Class/10708/  视频</span><br><span class="line">http://www.cs.cmu.edu/~epxing/Class/10701</span><br><span class="line">https://sites.google.com/site/10601a14spring/syllabus </span><br><span class="line">http://wenku.baidu.com/course/view/49e8b8f67c1cfad6195fa705</span><br></pre></td></tr></table></figure>

<p><strong>相关学术文章下载资源</strong></p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html</span><br><span class="line">CV:http://www.cvpapers.com/index.html; </span><br><span class="line"><span class="attribute">NIPS</span><span class="punctuation">: </span>http://books.nips.cc/; </span><br><span class="line">JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  </span><br></pre></td></tr></table></figure>

<p><strong>机器学习的定义</strong></p>
<p>机器学习有下面几种定义：<br>“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。<br>“机器学习是对能通过经验自动改进的计算机算法的研究”。<br>“机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。”<br>英文定义：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<p><strong>机器学习的应用</strong></p>
<ul>
<li>数据挖掘</li>
<li>计算机视觉</li>
<li>自然语言处理</li>
<li>生物特征识别</li>
<li>搜索引擎</li>
<li>医学诊断</li>
<li>检测信用卡欺诈</li>
<li>证券市场分析</li>
<li>DNA序列测序</li>
<li>语音和手写识别</li>
<li>战略游戏</li>
<li>机器人</li>
</ul>
<p><strong>机器学习新的方向</strong></p>
<ul>
<li>集成学习</li>
<li>可扩展机器学习（对大数据集、高维数据的学习等）</li>
<li>强化学习</li>
<li>迁移学习</li>
<li>概率网络</li>
<li>深度学习</li>
</ul>
<p><strong>机器学习和数据挖掘的关系</strong></p>
<ul>
<li>机器学习是数据挖掘的重要工具。</li>
<li>数据挖掘不仅仅要研究、拓展、应用一些机器学习方法，还要通过许多非机器学习技术解决数据仓储、大规模数据、数据噪音等等更为实际的问题。</li>
<li>机器学习的涉及面更宽，常用在数据挖掘上的方法通常只是“从数据学习”，然则机器学习不仅仅可以用在数据挖掘上，一些机器学习的子领域甚至与数据挖掘关系不大，例如增强学习与自动控制等等。</li>
<li>数据挖掘试图从海量数据中找出有用的知识。</li>
<li>大体上看，数据挖掘可以视为机器学习和数据库的交叉，它主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。</li>
</ul>
<p><strong>机器学习相关学术期刊和会议</strong></p>
<p><strong>机器学习</strong><br>    学术会议：NIPS、ICML、ECML和COLT，<br>    学术期刊：《Machine Learning》和《Journal of Machine Learning Research》<br><strong>数据挖掘</strong><br>    学术会议：SIGKDD、ICDM、SDM、PKDD和PAKDD<br>    学术期刊：《Data Mining and Knowledge Discovery》和《IEEE Transactions on Knowledge and Data Engineering》<br><strong>人工智能</strong><br>    学术会议：IJCAI和AAAI、<br><strong>数据库</strong><br>    学术会议：SIGMOD、VLDB、ICDE，<br><strong>其它一些顶级期刊如</strong><br>    《Artificial Intelligence》、<br>    《Journal of Artificial Intelligence Research》、<br>    《IEEE Transactions on Pattern Analysis and Machine Intelligence》、<br>    《Neural Computation》等也经常发表机器学习和数据挖掘方面的论文</p>
<h1 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h1><p>统计学习或机器学习一般包括<strong>监督学习</strong>、<strong>无监督学习</strong>、<strong>强化学习</strong>。有时还包括<strong>半监督学习</strong>、<strong>主动学习</strong>。</p>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>  <strong>监督学习（supervised learning）</strong> 是指从<strong>标注数据</strong>中学习预测模型的机器学习问题。<strong>标注数据</strong>表示输入输出的对应关系，<strong>预测模型</strong>对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的映射的统计规律。</p>
<p>（1）<strong>输入空间、输出空间和特征空间：</strong><br>          输入空间：输入所有可能取值的集合<br>          输出空间：输出所有可能取值的集合<br>          特征空间：所有特征向量存在的空间<br>  <strong>注1：</strong>输入与输出空间可以是有限元素的集合，也可以是整个欧式空间；输入空间与输出空间可以是同一个空间，也可以是不同的空间；通常输出空间远远小于输入空间。<br>  <strong>注2：</strong>特征空间的每一维对应一个特征。有时假设输入空间与特征空间为相同的空间，对它们不予区分；有时假设输入空间与特征空间为不同的空间，将实例从输入空间映射到特征空间。<br>（2）<strong>联合概率分布</strong><br>  监督学习假设输入与输出的随机变量X 和Y 遵循联合概率分布P ( X , Y ) 。P ( X , Y ) 表示分布函数，或分布密度函数。<br>  统计学习假设数据存在一定的统计规律，训练数据与测试数据被看作是依联合概率分布P ( X , Y ) 独立同分布产生的。<br>（3）<strong>假设空间</strong><br>  模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。假设空间也即意味着监督学习所要学习的范围。<br>（4）<strong>问题的形式化</strong><br>  监督学习分为学习和预测两个过程，由学习系统与预测系统共同完成。<br><img src="https://img-blog.csdnimg.cn/c8c90cfa48474a92b6c61d157376e940.png#pic_center" alt="img"></p>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p> 无监督学习（unsupervised learning） 是指从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据，预测模型表示数据的类别、转换或概率。无监督学习的本质是学习数据中的统计规律或潜在结构。<br>  无监督学习可用于对已有数据的分析，也可用于对未来数据的预测。它和监督学习有类似的流<br><img src="https://img-blog.csdnimg.cn/d4ffd967656844ad93ba49b905ebe50d.png#pic_center" alt="img"></p>
<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>​        <strong>强化学习（reinforcement learning）</strong> 是指<strong>智能系统与环境的连续互动</strong>中学习最优行为策略的机器学习问题。强化学习的本质是学习最优的序贯决策。<br>  强化学习过程中，<code>智能系统不断地试错，以达到学习最优策略的目的</code>。智能系统与环境的互动如图所示</p>
<p><img src="https://img-blog.csdnimg.cn/2bed1450aac74b85854540d424e3d58b.png#pic_center" alt="img"></p>
<h2 id="半监督学习与主动学习"><a href="#半监督学习与主动学习" class="headerlink" title="半监督学习与主动学习"></a>半监督学习与主动学习</h2><p>​        <strong>半监督学习（semi-supervised learning）</strong> 是指利用标注数据和未标注数据学习预测模型的机器学习问题。半监督学习旨在利用未标注数据中的信息，辅助标注数据进行监督学习，以较低的成本达到较好的学习效果。<br>  <strong>主动学习（active learning）</strong> 是指机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。主动学习旨在找出对学习最有帮助的实例让教师标注，以较小的标注代价，达到较好的学习效果。</p>
<h1 id="机器学习方法三要素"><a href="#机器学习方法三要素" class="headerlink" title="机器学习方法三要素"></a>机器学习方法三要素</h1><p>机器学习方法由<strong>模型</strong>、<strong>策略</strong>和<strong>算法</strong>三要素构成</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>​        机器学习首要考虑的问题是学习什么样的模型。在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。</p>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>​        有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。<br>（1）损失函数和风险函数<br>  损失函数：度量模型一次预测的好坏<br>  风险函数：度量平均意义下模型预测的好坏<br>由于模型的输入、输出( X , Y ) 是随机变量，遵循联合分布P ( X , Y ) ，所以损失函数的期望是：</p>
<p><img src="https://raw.githubusercontent.com/446773160/Picbed/main/blog_images20221021181635.png" alt="img"></p>
<p>​        这是理论模型f ( X )关于联合分布P ( X , Y ) 的平均意义下的损失，称为风险函数或期望损失。<br>实际上，联合分布P ( X , Y )是未知的，因为Rexp(f)是不能直接计算的，但根据大数定律，当样本容量N趋于无穷时，经验风险Remp(f)趋于期望风险Rexp(f)<strong>。</strong> 因为很自然的一个想法，即用经验风险估计期望风险。模型f ( X ) 关于训练数据集的平均损失即为经验风险或经验损失，记作Remp</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021182719.png?raw=true" alt="blog_images20221021182719.png"></p>
<p>但由于现实中训练样本数目有限，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。</p>
<p>（2）经验风险最小化与结构风险最小化<br>  为了求解到最优的模型，在监督学习中经常采用经验风险最小化和结构风险最小化这两个基本策略来选择模型。<br>  经验风险最小化（empirical risk minimization,ERM） 的策略认为，经验风险最小的模型即最优的模型，上述问题可转化为求解如下的最优化问题：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021183729.png?raw=true" alt="blog_images20221021183729.png"></p>
<p>注：F是假设空间<br>  当样本容量足够大时，经验风险最小化能保证有很好的学习效果。但是，当样本容量很小时，经验风险最小化学习的效果未必很好，会产生“过拟合”现象。<br>  结构风险最小化（structural risk minimization,SRM） 即为了防止过拟合而提出的策略。其通过在经验风险基础上加上表示模型复杂度的正则化项或惩罚项，在本质上等价于正则化。结构风险可定义如下：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021183729.png?raw=true" alt="blog_images20221021183729.png"></p>
<p>注：J(f)为模型的复杂度，是定义在假设空间F上的泛函<br>  结构风险最小化策略认为结构风险最小的模型是最优的模型，所以求最优模型就是求解最优化问题：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021183928.png?raw=true" alt="blog_images20221021183928.png"></p>
<p>     综上，监督学习问题就变成了经验风险或结构风险函数的最优化问题，经验风险或结构风险的函数就是最优化的目标函数。</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>算法指学习模型的具体计算方法。由于统计学问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。</p>
<h1 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h1><h2 id="训练误差与测试误差"><a href="#训练误差与测试误差" class="headerlink" title="训练误差与测试误差"></a>训练误差与测试误差</h2><p>  统计学习的目的是使学到的模型不仅对已知数据而且对未知数据都能有很好的预测能力。不同的学习方法会给出不同的模型，而基于损失函数的模型训练误差（training error） 和模型测试误差（test error） 就自然成为学习方法的评估的标准。<br>  训练误差的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要。测试误差反映了学习方法对未知的测试数据集的预测能力。通常将学习方法对未知数据的预测能力称为泛化能力。</p>
<h2 id="过拟合与模型选择"><a href="#过拟合与模型选择" class="headerlink" title="过拟合与模型选择"></a>过拟合与模型选择</h2><p>  当假设空间含有不同复杂度的模型时，就要面临模型选择的问题。如果在假设空间中存在“真”模型，那么所选择的模型应该逼近真模型。<br>  如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种现象称为过拟合（over-fitting）。这种现象表现为对已知数据预测得很好，但对为知数据预测得很差。</p>
<h1 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h1><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>  模型选择的典型方法是<strong>正则化（regularization）</strong>。正则化是结构风险最小化策略的实现，是<code>在经验风险上加一个正则化项或惩罚项</code>。一般具有如下形式：</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221021184148.png?raw=true" alt="blog_images20221021184148.png"></p>
<p>  利用正则化进行模型选择的方法符合奥卡姆剃刀原理，即在所有可能选择的模型中，<code>能够很好地解释已知数据并且十分简单</code>才是最好的模型，也就是应该选择的模型。</p>
<h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>​    另一种模型选择的典型方法是交叉验证（cross validation）。它的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择。</p>
<p>​    常见的交叉验证方法有以下三种：简单交叉验证、S折交叉验证、留一交叉验证</p>
<h1 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h1><p>​    学习方法的<strong>泛化能力（generalization ability）</strong> 是指由该方法学习到的模型<code>对未知数据的预测能力</code>，是学习方法本质上重要的性质。</p>
<h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><p>     监督学习方法又可以分为生成方法和判别方法，所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。<br>  生成方法由数据学习联合概率分布P ( X , Y ) ，然后求出条件概率分布P ( Y ∣ X ) 作为预测的模型（生成模型），之所以被称之为生成方法，是因为模型表示了给定输入X XX产生输出Y YY的生成关系。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">生成方法的特点：</span><br><span class="line">（1）生成方法可以还原出联合概率分布P ( X , Y ) ；</span><br><span class="line">（2）生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；</span><br><span class="line">（3）生成方法适用于存在隐变量的学习。</span><br></pre></td></tr></table></figure>

<p> 判别方法由数据直接学习决策函数f ( X ) 或者条件概率分布P ( Y ∣ X ) 作为预测模型（判别模型），判别方法关注于给定的输入X ，应该预测什么样的输出Y 。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">判别方法的特点：</span><br><span class="line">（1）判别方法直接学习条件概率P ( Y ∣ X ) 或决策函数f ( X ) ，直接面对预测，往往学习的准确率更高；</span><br><span class="line">（2）判别方法可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</span><br></pre></td></tr></table></figure>

<h1 id="监督学习应用"><a href="#监督学习应用" class="headerlink" title="监督学习应用"></a>监督学习应用</h1><p>​    监督学习的应用主要在三个方面：<strong>分类问题</strong>、<strong>标注问题</strong>和<strong>回归问题</strong>。</p>
<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p> 输入变量为有限个离散变量的预测问题称为分类问题。其表示如图</p>
<p><img src="https://img-blog.csdnimg.cn/65054a8e54ab43c3bb0d907e0e693756.png#pic_center" alt="img"></p>
<h2 id="标注问题"><a href="#标注问题" class="headerlink" title="标注问题"></a>标注问题</h2><p> 输入变量与输出变量均为变量序列的预测问题称为标注问题。其表示如图</p>
<p><img src="https://img-blog.csdnimg.cn/143fb3dc8f7d4574aaa88b2561f29e24.png#pic_center" alt="img"></p>
<h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>输入变量与输出变量均为连续变量的预测问题称为回归问题。其表示如图</p>
<p><img src="https://img-blog.csdnimg.cn/ae3d0da74c7d41cc9016f56894a49adf.png#pic_center" alt="img"></p>
<h1 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h1><h2 id="sklearn概述"><a href="#sklearn概述" class="headerlink" title="sklearn概述"></a>sklearn概述</h2><p>scikit-learn计划开始于scikits.learn，它是David Cournapeau（英语：David Cournapeau）的Google编程之夏计划。它的名字来源自成为“SciKit”（SciPy工具箱）的想法，即一个独立开发和发行的第三方SciPy扩展。最初的代码库被其他开发者重写了。在2010年，来自法国罗康库尔的法国国家信息与自动化研究所的Fabian Pedregosa、Gael Varoquaux、Alexandre Gramfort和Vincent Michel，领导了这个项目并在2010年2月1日进行了首次公开发行。在各种scikit中，scikit-learn和scikit-image（英语：scikit-image）截至2012年11月被称为“良好维护和流行的”。Scikit-learn是在GitHub上最流行的机器学习库之一。</p>
<h2 id="sklearn模块"><a href="#sklearn模块" class="headerlink" title="sklearn模块"></a>sklearn模块</h2><p><strong>注：sklearn里面只能接收二维的矩阵，如果只是一维请采用reshape(-1,1)</strong></p>
<p><strong>分类 (Classification)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> SomeClassifier	</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SomeClassifier	</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> SomeClassifier</span><br></pre></td></tr></table></figure>

<p><strong>回归 (Regression)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> SomeRegressor	</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SomeRegressor	</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> SomeRegressor</span><br></pre></td></tr></table></figure>

<p><strong>聚类 (Clustering)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p><strong>降维 (Dimensionality Reduction)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p><strong>模型选择 (Model Selection)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p><strong>预处理 (Preprocessing)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> SomeModel</span><br></pre></td></tr></table></figure>

<p>此外，Sklearn 里面还有很多自带数据集供，引入它们的伪代码如下。</p>
<p><strong>数据集 (Dataset)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> SomeData</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">datasets.load_boston <span class="comment">#波士顿房价数据集  回归模型</span></span><br><span class="line">datasets.load_breast_cancer <span class="comment">#乳腺癌数据集  分类模型</span></span><br><span class="line">datasets.load_diabetes <span class="comment">#糖尿病数据集  回归模型</span></span><br><span class="line">datasets.load_digits <span class="comment">#手写体数字数据集  分类模型</span></span><br><span class="line">datasets.load_files  </span><br><span class="line">datasets.load_iris <span class="comment">#鸢尾花数据集  分类模型</span></span><br><span class="line">datasets.load_lfw_pairs  </span><br><span class="line">datasets.load_lfw_people  </span><br><span class="line">datasets.load_linnerud <span class="comment">#体能训练数据集  回归模型</span></span><br><span class="line">datasets.load_mlcomp  </span><br><span class="line">datasets.load_sample_image  </span><br><span class="line">datasets.load_sample_images  </span><br><span class="line">datasets.load_svmlight_file  </span><br><span class="line">datasets.load_svmlight_files  </span><br></pre></td></tr></table></figure>

<h2 id="预处理-Preprocessing"><a href="#预处理-Preprocessing" class="headerlink" title="预处理(Preprocessing)"></a>预处理(Preprocessing)</h2><h3 id="数据无量纲化"><a href="#数据无量纲化" class="headerlink" title="数据无量纲化"></a>数据无量纲化</h3><p> 在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一规格，或不同分布的数据转换到某个特定分布 的需求，这种需求统称为将数据“无量纲化”。</p>
<p><strong>reprocessing.MinMaxScaler</strong></p>
<p>当数据data中的一个特征太大严重影响另外一个特征，数据data按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到 [0,1]之间，而这个过程，就叫做数据归一化(Normalization，又称Min-Max Scaling)。公式如下：<br>$$<br>x=\frac{x-min(x)}{max(x)-min(x)}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(data)</span><br><span class="line"><span class="comment">#实现归一化</span></span><br><span class="line">scaler = MinMaxScaler() <span class="comment">#实例化</span></span><br><span class="line">scaler = scaler.fit(data) <span class="comment">#fit，在这里本质是生成min(x)和max(x)</span></span><br><span class="line">result = scaler.transform(data) <span class="comment">#通过接口导出结果</span></span><br><span class="line">result</span><br><span class="line">result_ = scaler.fit_transform(data) <span class="comment">#训练和导出结果一步达成</span></span><br><span class="line">scaler.inverse_transform(result) <span class="comment">#将归一化后的结果逆转</span></span><br><span class="line"><span class="comment">#使用MinMaxScaler的参数feature_range实现将数据归一化到[0,1]以外的范围中</span></span><br><span class="line">data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line">scaler = MinMaxScaler(feature_range=[<span class="number">5</span>,<span class="number">10</span>]) <span class="comment">#依然实例化</span></span><br><span class="line">result = scaler.fit_transform(data) <span class="comment">#fit_transform一步导出结果</span></span><br><span class="line">result</span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.StandardScaler</strong></p>
<p> 当数据(x)按均值(μ)中心化后，再按标准差(σ)缩放，数据就会服从为均值为0，方差为1的正态分布（即标准正态分 布），而这个过程，就叫做数据标准化(Standardization）。公式如下<br>$$<br>x=\frac{x-\mu}{\sigma}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">data = [[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]]</span><br><span class="line">scaler = StandardScaler() <span class="comment">#实例化</span></span><br><span class="line">scaler.fit(data) <span class="comment">#fit，本质是生成均值和方差</span></span><br><span class="line">scaler.mean_ <span class="comment">#查看均值的属性mean_</span></span><br><span class="line">scaler.var_ <span class="comment">#查看方差的属性var_</span></span><br><span class="line">x_std = scaler.transform(data) <span class="comment">#通过接口导出结果</span></span><br><span class="line">x_std.mean() <span class="comment">#导出的结果是一个数组，用mean()查看均值</span></span><br><span class="line">x_std.std() <span class="comment">#用std()查看方差</span></span><br><span class="line">scaler.fit_transform(data) <span class="comment">#使用fit_transform(data)一步达成结果</span></span><br><span class="line">scaler.inverse_transform</span><br><span class="line">(x_std) <span class="comment">#使用inverse_transform逆转标准化</span></span><br></pre></td></tr></table></figure>

<p><strong>StandardScaler</strong>和<strong>MinMaxScaler</strong>选哪个？</p>
<p>​        看情况。大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。</p>
<p>​        MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。</p>
<p>​        建议先试试看StandardScaler，效果不好换MinMaxScaler。</p>
<p>​        除了StandardScaler和MinMaxScaler之外，sklearn中也提供了各种其他缩放处理（中心化只需要一个pandas广</p>
<p>播一下减去某个数就好了，因此sklearn不提供任何中心化功能）。比如，在希望压缩数据，却不影响数据的稀疏</p>
<p>性时（不影响矩阵中取值为0的个数时），我们会使用MaxAbsScaler；在异常值多，噪声非常大时，我们可能会选</p>
<p>用分位数来无量纲化，此时使用RobustScaler。更多详情请参考以下列表</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221022093634.png?raw=true" alt="blog_images20221022093634.png"></p>
<h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><p>机器学习和数据挖掘中所使用的数据，永远不可能是完美的。很多特征，对于分析和建模来说意义非凡，但对于实际收集数据的人却不是如此，因此数据挖掘之中，常常会有重要的字段缺失值很多，但又不能舍弃字段的情况。因此，数据预处理中非常重要的一项就是处理缺失值</p>
<p><strong>缺失值处理一般采用以下方式：<br>如果是数值类型，用平均值取代；<br>如果是分类数据，用最常见的类别取代；</strong></p>
<p><strong>impute.SimpleImputer</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.impute.SimpleImputer (missing_values=nan, strategy=’mean’, fill_value=<span class="literal">None</span>, verbose=<span class="number">0</span>,</span><br><span class="line">copy=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>含义&amp;输入</strong></th>
</tr>
</thead>
<tbody><tr>
<td>missing_values</td>
<td>告诉SimpleImputer，数据中的缺失值长什么样，默认空值np.nan</td>
</tr>
<tr>
<td>strategy</td>
<td>我们填补缺失值的策略，默认均值。输入“mean”使用均值填补（仅对数值型特征可用)                    输入“median”用中值填补（仅对数值型特征可用 ）                                                                          输入”most_frequent”用众数填补（对数值型和字符型特征都可用）                                                  输入“constant”表示请参考参数“fifill_value”中的值（对数值型和字符型特征都可用）</td>
</tr>
<tr>
<td>fifill_value</td>
<td>当参数startegy为”constant”的时候可用，可输入字符串或数字表示要填充的值，常用0</td>
</tr>
<tr>
<td>copy</td>
<td>默认为True，将创建特征矩阵的副本，反之则会将缺失值填补到原本的特征矩阵中去。</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Age = data.loc[:,<span class="string">&quot;Age&quot;</span>].values.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#sklearn当中特征矩阵必须是二维</span></span><br><span class="line">Age[:<span class="number">20</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line">imp_mean = SimpleImputer() <span class="comment">#实例化，默认均值填补</span></span><br><span class="line">imp_median = SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>) <span class="comment">#用中位数填补</span></span><br><span class="line">imp_0 = SimpleImputer(strategy=<span class="string">&quot;constant&quot;</span>,fill_value=<span class="number">0</span>) <span class="comment">#用0填补</span></span><br><span class="line">imp_mean = imp_mean.fit_transform(Age) <span class="comment">#fit_transform一步完成调取结果</span></span><br><span class="line">imp_median = imp_median.fit_transform(Age)</span><br><span class="line">imp_0 = imp_0.fit_transform(Age)</span><br></pre></td></tr></table></figure>

<p>补充:其实利用pandas和numpy进行填充更简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">r&quot;D:\Project\PythonProject\Python01\jupyter\train.csv&quot;</span>,index_col=<span class="number">0</span>)</span><br><span class="line">data.head()</span><br><span class="line">data.loc[:,<span class="string">&quot;Age&quot;</span>] = data.loc[:,<span class="string">&quot;Age&quot;</span>].fillna(data.loc[:,<span class="string">&quot;Age&quot;</span>].median())</span><br><span class="line"><span class="comment">#.fillna 在DataFrame里面直接进行填补</span></span><br><span class="line">data.dropna(axis=<span class="number">0</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#.dropna(axis=0)删除所有有缺失值的行，.dropna(axis=1)删除所有有缺失值的列</span></span><br><span class="line"><span class="comment">#参数inplace，为True表示在原数据集上进行修改，为False表示生成一个复制对象，不修改原数据，默认False</span></span><br></pre></td></tr></table></figure>

<h3 id="处理分类型特征：编码与哑变量"><a href="#处理分类型特征：编码与哑变量" class="headerlink" title="处理分类型特征：编码与哑变量"></a>处理分类型特征：编码与哑变量</h3><p>在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，除了专用来处理文字的算法，其他算法在fifit的时候全部要求输入数组或矩阵，也不能够导入文字型数据（其实手写决策树和普斯贝叶斯可以处理文字，但是sklearn中规定必须导入数值型）。然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是[“小学”，“初中”，“高中”，”大学”]，付费方式可能包含[“支付宝”，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行编码，即是说，将文字型数据转换为数值型。</p>
<p><strong>preprocessing.LabelEncoder</strong>：标签专用，能够将分类转换为分类数值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">y = data.iloc[:,-<span class="number">1</span>] <span class="comment">#要输入的是标签，不是特征矩阵，所以允许一维</span></span><br><span class="line">le = LabelEncoder() <span class="comment">#实例化</span></span><br><span class="line">le = le.fit(y) <span class="comment">#导入数据</span></span><br><span class="line">label = le.transform(y)   <span class="comment">#transform接口调取结果</span></span><br><span class="line">le.classes_ <span class="comment">#属性.classes_查看标签中究竟有多少类别</span></span><br><span class="line">label <span class="comment">#查看获取的结果label</span></span><br><span class="line">le.fit_transform(y) <span class="comment">#也可以直接fit_transform一步到位</span></span><br><span class="line">le.inverse_transform(label) <span class="comment">#使用inverse_transform可以逆转</span></span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.OrdinalEncoder</strong>：特征专用，能够将分类特征转换为分类数值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"><span class="comment">#接口categories_对应LabelEncoder的接口classes_，一模一样的功能</span></span><br><span class="line">data_ = data.copy()</span><br><span class="line">data_.head()</span><br><span class="line">OrdinalEncoder().fit(data_.iloc[:,<span class="number">1</span>:-<span class="number">1</span>]).categories_</span><br><span class="line">data_.iloc[:,<span class="number">1</span>:-<span class="number">1</span>] = OrdinalEncoder().fit_transform(data_.iloc[:,<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">data_.head()</span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.OneHotEncoder</strong>：独热编码，创建哑变量</p>
<p>我们刚才已经用OrdinalEncoder把分类变量Sex和Embarked都转换成数字对应的类别了。在舱门Embarked这一列中，我们使用[0,1,2]代表了三个不同的舱门，然而这种转换是正确的吗？</p>
<p>我们来思考三种不同性质的分类数据：</p>
<p>1） 舱门（S，C，Q）</p>
<p>三种取值S，C，Q是相互独立的，彼此之间完全没有联系，表达的是S≠C≠Q的概念。这是名义变量。</p>
<p>2） 学历（小学，初中，高中）</p>
<p>三种取值不是完全独立的，我们可以明显看出，在性质上可以有高中&gt;初中&gt;小学这样的联系，学历有高低，但是学历取值之间却不是可以计算的，我们不能说小学 + 某个取值 = 初中。这是有序变量。</p>
<p>3） 体重（&gt;45kg，&gt;90kg，&gt;135kg）</p>
<p>各个取值之间有联系，且是可以互相计算的，比如120kg - 45kg = 90kg，分类之间可以通过数学计算互相转换。这是有距变量。然而在对特征进行编码的时候，这三种分类数据都会被我们转换为[0,1,2]，这三个数字在算法看来，是连续且可以计算的，这三个数字相互不等，有大小，并且有着可以相加相乘的联系。所以算法会把舱门，学历这样的分类特征，都误会成是体重这样的分类特征。这是说，我们把分类转换成数字的时候，忽略了数字中自带的数学性质，所以给算法传达了一些不准确的信息，而这会影响我们的建模。类别OrdinalEncoder可以用来处理有序变量，但对于名义变量，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息。</p>
<p><img src="https://github.com/446773160/Picbed/blob/main/blog_images20221022152130.png?raw=true" alt="blog_images20221022152130.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">&#x27;auto&#x27;</span>).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line">result</span><br><span class="line"><span class="comment">#依然可以直接一步到位，但为了给大家展示模型属性，所以还是写成了三步</span></span><br><span class="line">OneHotEncoder(categories=<span class="string">&#x27;auto&#x27;</span>).fit_transform(X).toarray()</span><br><span class="line"><span class="comment">#依然可以还原</span></span><br><span class="line">pd.DataFrame(enc.inverse_transform(result))</span><br><span class="line">enc.get_feature_names()</span><br><span class="line">result</span><br><span class="line">result.shape</span><br><span class="line"><span class="comment">#axis=1,表示跨行进行合并，也就是将量表左右相连，如果是axis=0，就是将量表上下相连</span></span><br><span class="line">newdata = pd.concat([data,pd.DataFrame(result)],axis=<span class="number">1</span>)</span><br><span class="line">newdata.head()</span><br><span class="line">newdata.drop([<span class="string">&quot;Sex&quot;</span>,<span class="string">&quot;Embarked&quot;</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">newdata.columns = [<span class="string">&quot;Age&quot;</span>,<span class="string">&quot;Survived&quot;</span>,<span class="string">&quot;Female&quot;</span>,<span class="string">&quot;Male&quot;</span>,<span class="string">&quot;Embarked_C&quot;</span>,<span class="string">&quot;Embarked_Q&quot;</span>,<span class="string">&quot;Embarked_S&quot;</span>]</span><br></pre></td></tr></table></figure>

<h3 id="处理连续型特征：二值化与分段"><a href="#处理连续型特征：二值化与分段" class="headerlink" title="处理连续型特征：二值化与分段"></a>处理连续型特征：二值化与分段</h3><p><strong>sklearn.preprocessing.Binarizer</strong></p>
<p>根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。二值化是对文本计数数据的常见操作，分析人员可以决定仅考虑某种现象的存在与否。它还可以用作考虑布尔随机变量的估计器的预处理步骤（例如，使用贝叶斯设置中的伯努利分布建模）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>].values.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#类为特征专用，所以不能使用一维数组</span></span><br><span class="line">transformer = Binarizer(threshold=<span class="number">30</span>).fit_transform(X)</span><br><span class="line">transformer</span><br></pre></td></tr></table></figure>

<p><strong>preprocessing.KBinsDiscretizer</strong></p>
<p>这是将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。总共包含三个重要参数：</p>
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>含义&amp;输入</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>n_bins</strong></td>
<td>每个特征中分箱的个数，默认5，一次会被运用到所有导入的特征</td>
</tr>
<tr>
<td><strong>encode</strong></td>
<td>编码的方式，默认“onehot”                                                                                                                      “onehot”：做哑变量，之后返回一个稀疏矩阵，每一列是一个特征中的一个类别，含有该类别的样本表示为1，不含的表示为0                                                                                                                                                      “ordinal”：每个特征的每个箱都被编码为一个整数，返回每一列是一个特征，每个特征下含有不同整数编码的箱的矩阵                                                                                                                                                “onehot-dense”：做哑变量，之后返回一个密集数组。</td>
</tr>
<tr>
<td><strong>strategy</strong></td>
<td>用来定义箱宽的方式，默认”quantile”                                                                                                       “uniform”：表示等宽分箱，即每个特征中的每个箱的最大值之间的差为(特征.max() - 特征.min())/(n_bins)      “quantile”：表示等位分箱，即每个特征中的每个箱内的样本数量都相同                                                                    “kmeans”：表示按聚类分箱，每个箱中的值到最近的一维k均值聚类的簇心得距离都相同</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>].values.reshape(-<span class="number">1</span>,<span class="number">1</span>) </span><br><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">&#x27;ordinal&#x27;</span>, strategy=<span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line">est.fit_transform(X)</span><br><span class="line"><span class="comment">#查看转换后分的箱：变成了一列中的三箱</span></span><br><span class="line"><span class="built_in">set</span>(est.fit_transform(X).ravel())</span><br><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">&#x27;onehot&#x27;</span>, strategy=<span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line"><span class="comment">#查看转换后分的箱：变成了哑变量</span></span><br><span class="line">est.fit_transform(X).toarray()</span><br></pre></td></tr></table></figure>

<h3 id="Filter过滤法"><a href="#Filter过滤法" class="headerlink" title="Filter过滤法"></a>Filter过滤法</h3><p>过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法。它是根据各种统计检验中的分数以及相关性的各项指标来选择特征。</p>
<p><strong>方差过滤</strong></p>
<p> <strong>VarianceThreshold</strong></p>
<p>这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为<strong>0的特征。VarianceThreshold有重要参数threshold</strong>，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">selector = VarianceThreshold() <span class="comment">#实例化，不填参数默认方差为0</span></span><br><span class="line">X_var0 = selector.fit_transform(X) <span class="comment">#获取删除不合格特征之后的新特征矩阵</span></span><br><span class="line"><span class="comment">#也可以直接写成 X = VairanceThreshold().fit_transform(X)</span></span><br></pre></td></tr></table></figure>

<p>可以看见，我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，我们希望留下一半的特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数threshold的值输入就好了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)</span><br><span class="line">X.var().values</span><br><span class="line">np.median(X.var().values)</span><br><span class="line">X_fsvar.shape</span><br></pre></td></tr></table></figure>

<p>当特征是二分类时，特征的取值就是伯努利随机变量，这些变量的方差可以计算为：<br>$$<br>Var[x]=p(1-p)<br>$$<br>其中X是特征矩阵，p是二分类特征中的一类在这个特征中所占的概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征</span></span><br><span class="line">X_bvar = VarianceThreshold(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)).fit_transform(X)</span><br><span class="line">X_bvar.shape</span><br></pre></td></tr></table></figure>

<h3 id="相关性过滤"><a href="#相关性过滤" class="headerlink" title="相关性过滤"></a>相关性过滤</h3><p>方差挑选完毕之后，我们就要考虑下一个问题：相关性了。我们希望选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量信息。如果特征与标签无关，那只会白白浪费我们的计算内存，可能还会给模型带来噪音。在sklearn当中，我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息</p>
<p><strong>卡方过滤</strong></p>
<p>卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类<strong>feature_selection.chi2</strong>计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合<strong>feature_selection.SelectKBest</strong>这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。另外，如果卡方检验检测到某个特征中所有的值都相同，会提示我们使用方差先进行方差过滤。并且，刚才我们已经验证过，当我们使用方差过滤筛选掉一半的特征后，模型的表现时提升的。因此在这里，我们使用threshold=中位数时完成的方差过滤的数据来做卡方检验（如果方差过滤后模型的表现反而降低了，那我们就不会使用方差过滤后的数据，而是使用原数据）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="comment">#假设在这里我一直我需要300个特征</span></span><br><span class="line">X_fschi = SelectKBest(chi2, k=<span class="number">300</span>).fit_transform(X_fsvar, y)</span><br><span class="line">X_fschi.shape</span><br></pre></td></tr></table></figure>

<p>验证一下模型的效果如何：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()</span><br></pre></td></tr></table></figure>

<p>如果效果降低了那么说明k=300的时候删除了模型相关的有效特征。那如何设置一个最佳的k值呢？在现实数据中，数据量很大，模型很复杂的时候，我们也许不能先去跑一遍模型看。看效果，而是希望最开始就能够选择一个最优的超参数k。那第一个方法，就是我们之前提过的学习曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#======【TIME WARNING: 5 mins】======#</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">390</span>,<span class="number">200</span>,-<span class="number">10</span>):</span><br><span class="line">    X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">350</span>,<span class="number">200</span>,-<span class="number">10</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>通过图例扎到最好的效果</p>
<table>
<thead>
<tr>
<th>p值</th>
<th>&lt;=0.05或0.01</th>
<th>&gt;0.05或0.01</th>
</tr>
</thead>
<tbody><tr>
<td>数据差异</td>
<td>差异不是自然形成的</td>
<td>这些差异是很自然的样本误差</td>
</tr>
<tr>
<td>相关性</td>
<td>两组数据是相关的</td>
<td>两组数据是相互独立的</td>
</tr>
<tr>
<td>原假设</td>
<td>拒绝原假设，接受备择假设</td>
<td>接受原假设</td>
</tr>
</tbody></table>
<p>从特征工程的角度，我们希望选取卡方值很大，p值小于0.05的特征，即和标签是相关联的特征。而调用SelectKBest之前，我们可以直接从chi2实例化后的模型中获得各个特征所对应的卡方值和P值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chivalue, pvalues_chi = chi2(X_fsvar,y)</span><br><span class="line"><span class="comment">#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征：</span></span><br><span class="line">k = chivalue.shape[<span class="number">0</span>] - (pvalues_chi &gt; <span class="number">0.05</span>).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">#X_fschi = SelectKBest(chi2, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<p> <strong>F检验</strong></p>
<p>F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而F检验回归用于标签是连续型变量的数据。和卡方检验一样，这两个类需要和类SelectKBest连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的K。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式。F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，我们希望选取<strong>p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的</strong>，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。以F检验的分类为例，我们继续在数字数据集上来进行特征选择</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line">F, pvalues_f = f_classif(X_fsvar,y)</span><br><span class="line">k = F.shape[<span class="number">0</span>] - (pvalues_f &gt; <span class="number">0.05</span>).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">#X_fsF = SelectKBest(f_classif, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsF,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<p>得到的结论和我们用卡方过滤得到的结论一模一样：没有任何特征的p值大于0.01，所有的特征都是和标签相关的，因此我们不需要相关性过滤。</p>
<p><strong>互信息法</strong></p>
<p>互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和feature_selection.mutual_info_regression（互信息回归）。这两个类的用法和参数都和F检验一模一样 ，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系。互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。以互信息分类为例的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_classif <span class="keyword">as</span> MIC</span><br><span class="line">result = MIC(X_fsvar,y) <span class="comment">#X_fsvar为特征,y为标签</span></span><br><span class="line">k = result.shape[<span class="number">0</span>] - <span class="built_in">sum</span>(result &lt;= <span class="number">0</span>)</span><br><span class="line"><span class="comment">#X_fsmic = SelectKBest(MIC, k=填写具体的k).fit_transform(X_fsvar, y)</span></span><br><span class="line"><span class="comment">#cross_val_score(RFC(n_estimators=10,random_state=0),X_fsmic,y,cv=5).mean()</span></span><br></pre></td></tr></table></figure>

<p>所有特征的互信息量估计都大于0，因此所有特征都与标签相关。当然了，无论是F检验还是互信息法，大家也都可以使用学习曲线，只是使用统计量的方法会更加高效。当统计量判断已经没有特征可以删除时，无论用学习曲线如何跑，删除特征都只会降低模型的表现。当然了，如果数据量太庞大，模型太复杂，我们还是可以牺牲模型表现来提升模型速度，一切都看大家的具体需求。</p>
<h3 id="Embedded嵌入法"><a href="#Embedded嵌入法" class="headerlink" title="Embedded嵌入法"></a>Embedded嵌入法</h3><p>嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性，比如决策树和树的集成模型中的feature_importances_属性，可以列出各个特征对树的建立的贡献，我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。因此相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.feature_selection.SelectFromModel (estimator, threshold=<span class="literal">None</span>, prefit=<span class="literal">False</span>, norm_order=<span class="number">1</span>,</span><br><span class="line">max_features=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>参数 说明</p>
<p><strong>estimator</strong> </p>
<p>使用的模型评估器，只要是带feature_importances_或者coef_属性，或带有l1和l2惩罚项的模型都可以使用</p>
<p><strong>threshold</strong> </p>
<p>特征重要性的阈值，重要性低于这个阈值的特征都将被删除</p>
<p><strong>prefifit</strong></p>
<p>默认False，判断是否将实例化后的模型直接传递给构造函数。如果为True，则必须直接调用fifit和transform，不能使用fifit_transform，并且SelectFromModel不能与cross_val_score，GridSearchCV和克隆估计器的类似实用程序一起使用。</p>
<p><strong>norm_order</strong></p>
<p>k可输入非零整数，正无穷，负无穷，默认值为1 ,在评估器的coef_属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数。</p>
<p><strong>max_features</strong> </p>
<p>在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据max_features选择，请设置threshold = -np.inf</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">X_embedded = SelectFromModel(RFC_,threshold=<span class="number">0.005</span>).fit_transform(X,y)</span><br><span class="line"><span class="comment">#在这里我只想取出来有限的特征。0.005这个阈值对于有780个特征的数据来说，是非常高的阈值，因为平均每个特征只能够分到大约0.001的feature_importances_</span></span><br></pre></td></tr></table></figure>

<h3 id="Wrapper包装法"><a href="#Wrapper包装法" class="headerlink" title="Wrapper包装法"></a>Wrapper包装法</h3><p>包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如coef_属性或feature_importances_属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。然后，从当前的一组特征中修剪最不重要的特征。在修剪的集合上递归地重复该过程，直到最终到达所需数量的要选择的特征。区别于过滤法和嵌入法的一次训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.feature_selection.RFE (estimator, n_features_to_select=<span class="literal">None</span>, step=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>参数<strong>estimator</strong>是需要填写的实例化后的评估器，<strong>n_features_to_select</strong>是想要选择的特征个数，<strong>step</strong>表示每次迭代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，**.support_<strong>：返回所有的特征的是否最后被选中的布尔矩阵，以及</strong>.ranking_**返回特征的按数次迭代中综合重要性的排名。类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">selector = RFE(RFC_, n_features_to_select=<span class="number">340</span>, step=<span class="number">50</span>).fit(X, y)</span><br></pre></td></tr></table></figure>



<h2 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h2><p><strong>算法</strong></p>
<p>遍历所有的样本点，计算每个样本点与待分类数据的距离，找出k个距离最近的点，统计每个类别的个数，投票数据最多的类别即为样本点的类别。</p>
<p>sklearn提供的模块</p>
<table>
<thead>
<tr>
<th>类方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>KNeighborsClassifier</td>
<td>KNN 算法解决分类问题</td>
</tr>
<tr>
<td>KNeighborsRegressor</td>
<td>KNN 算法解决回归问题</td>
</tr>
<tr>
<td>RadiusNeighborsClassifier</td>
<td>基于半径来查找最近邻的分类算法</td>
</tr>
<tr>
<td>NearestNeighbors</td>
<td>基于无监督学习实现KNN算法</td>
</tr>
<tr>
<td>KDTree</td>
<td>无监督学习下基于 KDTree 来查找最近邻的分类算法</td>
</tr>
<tr>
<td>BallTree</td>
<td>无监督学习下基于 BallTree 来查找最近邻的分类算法</td>
</tr>
</tbody></table>
<p><strong>sklearn.neighbors.KNeighborsClassifier参数说明</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier ( n_neighbors = <span class="number">5</span> , * , weights = <span class="string">&#x27;uniform&#x27;</span> , algorithm = <span class="string">&#x27;auto&#x27;</span> , leaf_size = <span class="number">30</span> , p = <span class="number">2</span> , metric = <span class="string">&#x27;minkowski&#x27;</span> , metric_params = <span class="literal">None</span> , n_jobs = <span class="literal">None</span> )</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>n_neighbors int，默认=5</strong></p>
<p>默认情况下用于kneighbors查询的邻居数。</p>
<p><strong>weights{‘uniform’, ‘distance’} ，默认=’uniform’</strong></p>
<p>预测中使用的权重函数。可能的值：</p>
<ul>
<li>‘uniform’ ：统一的权重。每个邻域中的所有点的权重相等。</li>
<li>‘distance’ ：权重点的距离的倒数。在这种情况下，查询点的较近的邻居将比较远的邻居具有更大的影响。</li>
<li>[callable] ：一个用户定义的函数，它接受一个距离数组，并返回一个包含权重的相同形状的数组。</li>
</ul>
<p><strong>algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’</strong></p>
<p>用于计算最近邻的算法：</p>
<ul>
<li>‘ball_tree’ 将使用BallTree</li>
<li>‘kd_tree’ 将使用KDTree</li>
<li>‘brute’ 将使用暴力搜索。</li>
<li>‘auto’ 将尝试根据传递给fit方法的值来决定最合适的算法。</li>
</ul>
<p>注意：拟合稀疏输入将使用蛮力覆盖此参数的设置。</p>
<p><strong>Leaf_size int，默认=30</strong></p>
<p>叶大小传递给 BallTree 或 KDTree。这会影响构建和查询的速度，以及存储树所需的内存。最佳值取决于问题的性质。</p>
<p><strong>p int，默认=2</strong></p>
<p>Minkowski 度量的功率参数。当 p = 1 时，这相当于使用 manhattan_distance (l1)，而 p = 2 则使用 euclidean_distance (l2)。对于任意 p，使用 minkowski_distance (l_p)。</p>
<p><strong>metric ：str 或可调用，默认=’minkowski’</strong></p>
<p>用于距离计算的度量。默认为“minkowski”，当 p = 2 时产生标准欧几里德距离。请参阅scipy.spatial.distance的文档和列出 distance_metrics的有效度量值的度量。</p>
<p>如果 metric 是“预先计算的”，则 X 被假定为一个距离矩阵，并且在拟合期间必须是平方的。X 可能是一个稀疏图，在这种情况下，只有“非零”元素可以被认为是邻居。</p>
<p>如果 metric 是一个可调用函数，它需要两个表示一维向量的数组作为输入，并且必须返回一个值来指示这些向量之间的距离。这适用于 Scipy 的指标，但效率低于将指标名称作为字符串传递。</p>
<p><strong>metric_params:dic，默认=无</strong></p>
<p>度量函数的附加关键字参数。</p>
<p><strong>n_jobs :int</strong>，默认=无</p>
</blockquote>
<p>属性：</p>
<blockquote>
<p><strong>Effective_metric_str 或callble</strong></p>
<p>使用的距离度量。它将与<code>metric</code>参数或其同义词相同，例如，如果<code>metric</code>参数设置为“minkowski”且<code>p</code>参数设置为 2，则为“euclidean”。</p>
<p>*<strong>Effective_metric_params_dict _：</strong></p>
<p>度量函数的附加关键字参数。对于大多数指标将与参数相同，但如果属性设置为“minkowski” <code>metric_params</code>，也可能包含 <code>p</code>参数值。<code>effective_metric_</code></p>
<p><strong>n_features_in_ int</strong></p>
<p>拟合期间看到的特征数。</p>
<p><em>0.24 版中的新功能。</em></p>
<p><strong>feature_names_in_ndarray 的形状 ( <code>n_features_in_</code>,)</strong></p>
<p>拟合期间看到的特征名称。仅当<code>X</code> 具有全为字符串的特征名称时才定义。</p>
<p><em>1.0 版中的新功能。</em></p>
<p><strong>n_samples_fit_int_</strong></p>
<p>拟合数据中的样本数。</p>
<p><strong>outputs_2d_bool</strong></p>
<p>当<code>y</code>‘ 的形状在拟合期间为 (n_samples, ) 或 (n_samples, 1) 时为假，否则为真。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载红酒数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line"><span class="comment">#KNN分类算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment">#分割训练集与测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment">#导入numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#加载数据集</span></span><br><span class="line">wine_dataset=load_wine()</span><br><span class="line"><span class="comment">#查看数据集对应的键</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;红酒数据集的键:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(wine_dataset.keys()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据集描述:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(wine_dataset[<span class="string">&#x27;data&#x27;</span>].shape))</span><br><span class="line"><span class="comment"># data 为数据集数据;target 为样本标签</span></span><br><span class="line"><span class="comment">#分割数据集，比例为 训练集：测试集 = 8:2</span></span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(wine_dataset[<span class="string">&#x27;data&#x27;</span>],wine_dataset[<span class="string">&#x27;target&#x27;</span>],test_size=<span class="number">0.2</span>,random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#构建knn分类模型，并指定 k 值</span></span><br><span class="line">KNN=KNeighborsClassifier(n_neighbors=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#使用训练集训练模型</span></span><br><span class="line">KNN.fit(X_train,y_train)</span><br><span class="line"><span class="comment">#评估模型的得分</span></span><br><span class="line">score=KNN.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line"><span class="comment">#给出一组数据对酒进行分类</span></span><br><span class="line">X_wine_test=np.array([[<span class="number">11.8</span>,<span class="number">4.39</span>,<span class="number">2.39</span>,<span class="number">29</span>,<span class="number">82</span>,<span class="number">2.86</span>,<span class="number">3.53</span>,<span class="number">0.21</span>,<span class="number">2.85</span>,<span class="number">2.8</span>,<span class="number">.75</span>,<span class="number">3.78</span>,<span class="number">490</span>]])</span><br><span class="line">predict_result=KNN.predict(X_wine_test)</span><br><span class="line"><span class="built_in">print</span>(predict_result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分类结果：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(wine_dataset[<span class="string">&#x27;target_names&#x27;</span>][predict_result]))</span><br></pre></td></tr></table></figure>



<h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>信息熵</strong>：决策树学习的关键是如何选择最优划分属性。划分过程中，决策树的分支结点所包含的样本尽可能属于同一类别，结点的“纯度”（purity）越来越高。信息熵是度量样本集合纯度最常用的一种指标。假定当前样本集合D DD中第k kk类样本所占的比例来为Pk (k=1,2,…,|y|)，则D的信息熵定义为</p>
<p>函数原型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.tree.DecisionTreeClassifier(*, criterion=<span class="string">&#x27;gini&#x27;</span>, splitter=<span class="string">&#x27;best&#x27;</span>, max_depth=<span class="literal">None</span>, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, class_weight=<span class="literal">None</span>, ccp_alpha=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>criterion ： gini或者entropy，前者是基尼指数，后者是信息熵；</li>
<li>max_depth ： int or None, optional (default=None) 设置决策随机森林中的决策树的最大深度，深度越大，越容易过合，推荐树的深度为：5-20之间；</li>
<li>max_features： None（所有），log2，sqrt，N 特征小于50的时候一般使用所有的；</li>
<li>max_leaf_nodes ： 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pandas用于处理和分析数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="comment"># 导入决策树分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="comment"># # 导入分割数据集的方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># import relevant packages</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_text</span><br><span class="line"><span class="comment"># import matplotlib; matplotlib.use(&#x27;TkAgg&#x27;)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用决策树进行鸢尾花数据集分类预测</span></span><br><span class="line"><span class="comment"># 数据集字段说明：</span></span><br><span class="line"><span class="comment"># 特征值（4个）：sepal length（花萼长度），sepal width（花萼宽度）， petal length（花瓣长度），petal width（花瓣宽度）</span></span><br><span class="line"><span class="comment"># 目标值（3个）：target（类别，0为&#x27;setosa&#x27;山鸢尾花，1为&#x27;versicolor&#x27;变色鸢尾花，2为&#x27;virginica&#x27;维吉尼亚鸢尾花）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load in the data加载数据</span></span><br><span class="line">data = load_iris()</span><br><span class="line"><span class="comment"># convert to a dataframe 转换数据格式</span></span><br><span class="line">df = pd.DataFrame(data.data, columns = data.feature_names)</span><br><span class="line"><span class="comment"># create the species column</span></span><br><span class="line">df[<span class="string">&#x27;Species&#x27;</span>] = data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># replace this with the actual names</span></span><br><span class="line">target = np.unique(data.target)  <span class="comment"># 对于一维数组或者列表，unique函数去除其中重复的元素，并按元素由大到小返回一个新的无元素重复的元组或者列表</span></span><br><span class="line">target_names = np.unique(data.target_names)</span><br><span class="line">targets = <span class="built_in">dict</span>(<span class="built_in">zip</span>(target, target_names))</span><br><span class="line">df[<span class="string">&#x27;Species&#x27;</span>] = df[<span class="string">&#x27;Species&#x27;</span>].replace(targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract features and target variables 提取特征和目标变量</span></span><br><span class="line">x = df.drop(columns=<span class="string">&quot;Species&quot;</span>)</span><br><span class="line">y = df[<span class="string">&quot;Species&quot;</span>]</span><br><span class="line"><span class="comment"># save the feature name and target variables 保存特征名称和目标变量</span></span><br><span class="line">feature_names = x.columns</span><br><span class="line">labels = y.unique()  <span class="comment"># 去除重复元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割训练集、测试集</span></span><br><span class="line"><span class="comment"># x 数据集的特征值</span></span><br><span class="line"><span class="comment"># y 数据集的标签值</span></span><br><span class="line"><span class="comment"># 训练集的特征值x_train 测试集的特征值x_test(test_x) 训练集的目标值y_train 测试集的目标值y_test(test_lab)</span></span><br><span class="line"><span class="comment"># random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</span></span><br><span class="line">X_train, test_x, y_train, test_lab = train_test_split(x,y,</span><br><span class="line">                                                 test_size = <span class="number">0.4</span>,</span><br><span class="line">                                                 random_state = <span class="number">42</span>)</span><br><span class="line"><span class="comment"># 创建决策树分类器（树的最大深度为3）</span></span><br><span class="line">model = DecisionTreeClassifier(max_depth =<span class="number">3</span>, random_state = <span class="number">42</span>)  <span class="comment"># 初始化模型</span></span><br><span class="line">model.fit(X_train, y_train)  <span class="comment"># 训练模型</span></span><br><span class="line"><span class="built_in">print</span>(model.score(test_x,test_lab))  <span class="comment"># 评估模型分数</span></span><br><span class="line"><span class="comment"># 计算每个特征的重要程度</span></span><br><span class="line"><span class="built_in">print</span>(model.feature_importances_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化特征属性结果</span></span><br><span class="line">r = export_text(model, feature_names=data[<span class="string">&#x27;feature_names&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt the figure, setting a black background</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>,<span class="number">10</span>), facecolor =<span class="string">&#x27;g&#x27;</span>)  <span class="comment"># facecolor设置背景色</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create the tree plot 决策树绘图模块，实现决策树可视化</span></span><br><span class="line">a = tree.plot_tree(model,</span><br><span class="line">                   <span class="comment"># use the feature names stored</span></span><br><span class="line">                   feature_names = feature_names,</span><br><span class="line">                   <span class="comment"># use the class names stored</span></span><br><span class="line">                   class_names = labels,</span><br><span class="line">                   <span class="comment"># label=&#x27;all&#x27;,</span></span><br><span class="line">                   rounded = <span class="literal">True</span>,</span><br><span class="line">                   filled = <span class="literal">True</span>,</span><br><span class="line">                   fontsize=<span class="number">14</span>,</span><br><span class="line">                   )</span><br><span class="line"><span class="comment"># show the plot</span></span><br><span class="line"><span class="comment"># plt.legend(loc=&#x27;lower right&#x27;, borderpad=0, handletextpad=0)</span></span><br><span class="line">plt.savefig(<span class="string">&quot;save.png&quot;</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">&quot;tight&quot;</span>)</span><br><span class="line"><span class="comment"># plt.tight_layout()</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>









































</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">山不让尘，川不辞盈</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">http://example.com/2022/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">山不让尘，川不辞盈</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/21/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"><img class="prev-cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">数据预处理</div></div></a></div><div class="next-post pull-right"><a href="/2022/10/19/QUIC%E5%8D%8F%E8%AE%AE/"><img class="next-cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">QUIC协议</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81NzQzNS8zMzg5OQ=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F14071825039%2F641&amp;refer=http%3A%2F%2Finews.gtimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668341762&amp;t=8f1764bb17a8a475a7658fd817bf1965" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">山不让尘，川不辞盈</div><div class="author-info__description">归途也还可爱</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/446773160"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/446773160" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/446773160@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_43762551?type=blog" target="_blank" title="CSDN"><i class="fa-solid fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到山不让尘,川不辞盈的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E6%8F%90"><span class="toc-number">1.</span> <span class="toc-text">前提</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">什么是机器学习？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">3.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.1.</span> <span class="toc-text">监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.2.</span> <span class="toc-text">无监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.3.</span> <span class="toc-text">强化学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.</span> <span class="toc-text">半监督学习与主动学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B8%89%E8%A6%81%E7%B4%A0"><span class="toc-number">4.</span> <span class="toc-text">机器学习方法三要素</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5"><span class="toc-number">4.2.</span> <span class="toc-text">策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">5.</span> <span class="toc-text">模型评估与模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="toc-number">5.1.</span> <span class="toc-text">训练误差与测试误差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">5.2.</span> <span class="toc-text">过拟合与模型选择</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">6.</span> <span class="toc-text">正则化与交叉验证</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">6.2.</span> <span class="toc-text">交叉验证</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-number">7.</span> <span class="toc-text">泛化能力</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.1.</span> <span class="toc-text">生成模型与判别模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8"><span class="toc-number">8.</span> <span class="toc-text">监督学习应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">8.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98"><span class="toc-number">8.2.</span> <span class="toc-text">标注问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">8.3.</span> <span class="toc-text">回归问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sklearn"><span class="toc-number">9.</span> <span class="toc-text">sklearn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn%E6%A6%82%E8%BF%B0"><span class="toc-number">9.1.</span> <span class="toc-text">sklearn概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sklearn%E6%A8%A1%E5%9D%97"><span class="toc-number">9.2.</span> <span class="toc-text">sklearn模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86-Preprocessing"><span class="toc-number">9.3.</span> <span class="toc-text">预处理(Preprocessing)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96"><span class="toc-number">9.3.1.</span> <span class="toc-text">数据无量纲化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">9.3.2.</span> <span class="toc-text">缺失值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%9E%8B%E7%89%B9%E5%BE%81%EF%BC%9A%E7%BC%96%E7%A0%81%E4%B8%8E%E5%93%91%E5%8F%98%E9%87%8F"><span class="toc-number">9.3.3.</span> <span class="toc-text">处理分类型特征：编码与哑变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E8%BF%9E%E7%BB%AD%E5%9E%8B%E7%89%B9%E5%BE%81%EF%BC%9A%E4%BA%8C%E5%80%BC%E5%8C%96%E4%B8%8E%E5%88%86%E6%AE%B5"><span class="toc-number">9.3.4.</span> <span class="toc-text">处理连续型特征：二值化与分段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Filter%E8%BF%87%E6%BB%A4%E6%B3%95"><span class="toc-number">9.3.5.</span> <span class="toc-text">Filter过滤法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7%E8%BF%87%E6%BB%A4"><span class="toc-number">9.3.6.</span> <span class="toc-text">相关性过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedded%E5%B5%8C%E5%85%A5%E6%B3%95"><span class="toc-number">9.3.7.</span> <span class="toc-text">Embedded嵌入法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Wrapper%E5%8C%85%E8%A3%85%E6%B3%95"><span class="toc-number">9.3.8.</span> <span class="toc-text">Wrapper包装法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN%E7%AE%97%E6%B3%95"><span class="toc-number">9.4.</span> <span class="toc-text">KNN算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">9.5.</span> <span class="toc-text">决策树算法</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/11/15/javaweb%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA(CentOS%E7%89%88%E6%9C%AC)/" title="javaweb开发环境搭建(CentOS版本)"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="javaweb开发环境搭建(CentOS版本)"/></a><div class="content"><a class="title" href="/2022/11/15/javaweb%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA(CentOS%E7%89%88%E6%9C%AC)/" title="javaweb开发环境搭建(CentOS版本)">javaweb开发环境搭建(CentOS版本)</a><time datetime="2022-11-15T10:00:00.000Z" title="发表于 2022-11-15 18:00:00">2022-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/14/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/" title="常见错误汇总"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="常见错误汇总"/></a><div class="content"><a class="title" href="/2022/11/14/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/" title="常见错误汇总">常见错误汇总</a><time datetime="2022-11-14T10:00:00.000Z" title="发表于 2022-11-14 18:00:00">2022-11-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/10/Pytorch/" title="Pytorch"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch"/></a><div class="content"><a class="title" href="/2022/11/10/Pytorch/" title="Pytorch">Pytorch</a><time datetime="2022-11-10T06:00:00.000Z" title="发表于 2022-11-10 14:00:00">2022-11-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/24/python/" title="无题"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/10/24/python/" title="无题">无题</a><time datetime="2022-10-24T14:28:24.485Z" title="发表于 2022-10-24 22:28:24">2022-10-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/21/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" title="数据挖掘"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据挖掘"/></a><div class="content"><a class="title" href="/2022/10/21/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" title="数据挖掘">数据挖掘</a><time datetime="2022-10-21T08:00:00.000Z" title="发表于 2022-10-21 16:00:00">2022-10-21</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fup.enterdesk.com%2Fedpic%2Ff8%2F1d%2F3e%2Ff81d3ebbd84122251e5403241aa4487b.jpg&amp;refer=http%3A%2F%2Fup.enterdesk.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=auto?sec=1668339555&amp;t=953420e9c4344774da896a7388445605')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 山不让尘，川不辞盈</div><div class="footer_custom_text">Hi, welcome to My Blog</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><div class="aplayer no-destroy" data-id="7422861869" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-lrcType="-1"> </div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "https://446773160.github.io/";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (true) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>